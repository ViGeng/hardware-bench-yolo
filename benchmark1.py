#!/usr/bin/env python3
"""
YOLOv8 æ€§èƒ½åŸºå‡†æµ‹è¯•å·¥å…· - æ”¹è¿›ç‰ˆ
ä¸»è¦æ”¹è¿›ï¼š
1. æ”¯æŒå‘½ä»¤è¡Œå‚æ•°ï¼Œä¸ç”¨ä¿®æ”¹ä»£ç 
2. æ”¯æŒå¤šç§è¾“å‡ºæ ¼å¼ï¼ˆtxt, json, csvï¼‰
3. æ›´å¥½çš„é”™è¯¯å¤„ç†
4. è‡ªåŠ¨æ£€æµ‹æ–‡ä»¶å’Œè®¾å¤‡
5. å¯é€‰æ˜¾ç¤ºæ£€æµ‹ç»“æœï¼ˆåƒåŸç‰ˆä¸€æ ·ï¼‰
6. å¯é€‰æ˜¾ç¤ºè¿›åº¦ä¿¡æ¯
"""

import argparse
import json
import csv
from ultralytics import YOLO
import numpy as np
import time
import psutil
import torch
import threading
import socket
from collections import deque
import os
import sys
from pathlib import Path

try:
    import pynvml
    NVML_AVAILABLE = True
except ImportError:
    NVML_AVAILABLE = False
    print("Warning: pynvml not available, GPU monitoring disabled")

def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='YOLOv8 Performance Benchmark Tool')
    
    # ä¸»è¦å‚æ•°
    parser.add_argument('--model', '-m', default='yolov8n.pt',
                        help='æ¨¡å‹è·¯å¾„æˆ–åç§° (é»˜è®¤: yolov8n.pt)')
    parser.add_argument('--source', '-s', default='0',
                        help='è§†é¢‘æº: æ–‡ä»¶è·¯å¾„æˆ–æ‘„åƒå¤´è®¾å¤‡å· (é»˜è®¤: 0)')
    
    # å¯é€‰å‚æ•°
    parser.add_argument('--device', '-d', default='auto',
                        help='è¿è¡Œè®¾å¤‡: cpu, cuda:0, auto (é»˜è®¤: auto)')
    parser.add_argument('--batch-size', '-b', type=int, default=1,
                        help='æ‰¹æ¬¡å¤§å° (é»˜è®¤: 1)')
    parser.add_argument('--max-frames', '-f', type=int,
                        help='æœ€å¤§å¤„ç†å¸§æ•° (é»˜è®¤: æ— é™åˆ¶)')
    parser.add_argument('--imgsz', type=int,
                        help='è¾“å…¥å›¾åƒå°ºå¯¸ (é»˜è®¤: æ¨¡å‹é»˜è®¤å€¼)')
    
    # è¾“å‡ºé€‰é¡¹
    parser.add_argument('--output-format', '-o', nargs='+', 
                        choices=['txt', 'json', 'csv'], default=['txt'],
                        help='è¾“å‡ºæ ¼å¼ (é»˜è®¤: txt)')
    parser.add_argument('--verbose', '-v', action='store_true',
                        help='æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯')
    parser.add_argument('--show-detections', action='store_true',
                        help='æ˜¾ç¤ºæ¯å¸§æ£€æµ‹ç»“æœ')
    parser.add_argument('--show-progress', action='store_true',
                        help='æ˜¾ç¤ºå¤„ç†è¿›åº¦ï¼ˆæ¯10å¸§ï¼‰')
    
    return parser.parse_args()

class BenchmarkTool:
    def __init__(self, args):
        self.args = args
        self.model = None
        self.monitoring = True
        
        # ç³»ç»Ÿä¿¡æ¯
        self.cpu_count = psutil.cpu_count()
        self.cpu_logical_count = psutil.cpu_count(logical=True)
        
        # åˆå§‹åŒ–æŒ‡æ ‡æ”¶é›†
        self.preprocess_times = []
        self.inference_times = []
        self.postprocess_times = []
        self.total_frames = 0
        self.start_time = None
        
        # åˆå§‹åŒ–èµ„æºç›‘æ§
        self.cpu_percentages = deque(maxlen=1000)
        self.memory_usages = deque(maxlen=1000)
        self.gpu_mem_usages = deque(maxlen=1000) if torch.cuda.is_available() else None
        self.gpu_util_usages = deque(maxlen=1000) if torch.cuda.is_available() else None
        
        self.monitor_thread = None
        self.nvml_handle = None
    
    def print_detection_results(self, result, frame_num):
        """æ˜¾ç¤ºæ£€æµ‹ç»“æœ"""
        try:
            if result.boxes is not None and len(result.boxes) > 0:
                # è·å–æ£€æµ‹åˆ°çš„ç±»åˆ«
                class_ids = result.boxes.cls.cpu().numpy().astype(int)
                confidences = result.boxes.conf.cpu().numpy()
                
                # ç»Ÿè®¡æ¯ç§ç‰©å“çš„æ•°é‡
                detections = {}
                for cls_id, conf in zip(class_ids, confidences):
                    class_name = result.names[cls_id]
                    if class_name not in detections:
                        detections[class_name] = []
                    detections[class_name].append(conf)
                
                # æ ¼å¼åŒ–è¾“å‡º
                if detections:
                    detection_strs = []
                    for class_name, confs in detections.items():
                        count = len(confs)
                        avg_conf = sum(confs) / count
                        if count == 1:
                            detection_strs.append(f"{class_name}({avg_conf:.2f})")
                        else:
                            detection_strs.append(f"{class_name}x{count}({avg_conf:.2f})")
                    
                    print(f"Frame {frame_num}: {', '.join(detection_strs)}")
                else:
                    print(f"Frame {frame_num}: No objects detected")
            else:
                print(f"Frame {frame_num}: No objects detected")
                
        except Exception as e:
            if self.args.verbose:
                print(f"Frame {frame_num}: Detection display error: {e}")
            else:
                print(f"Frame {frame_num}: -")
        
    def load_model(self):
        """åŠ è½½YOLOæ¨¡å‹"""
        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {self.args.model}")
        try:
            self.model = YOLO(self.args.model)
            print(f"æ¨¡å‹åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            sys.exit(1)
    
    def validate_source(self):
        """éªŒè¯è¾“å…¥æº"""
        if self.args.source.isdigit():
            print(f"ä½¿ç”¨æ‘„åƒå¤´è®¾å¤‡: {self.args.source}")
            return int(self.args.source)
        elif os.path.exists(self.args.source):
            print(f"ä½¿ç”¨è§†é¢‘æ–‡ä»¶: {self.args.source}")
            return self.args.source
        else:
            print(f"é”™è¯¯: æ‰¾ä¸åˆ°è¾“å…¥æº '{self.args.source}'")
            sys.exit(1)
    
    def check_and_fix_device(self):
        """æ£€æŸ¥å¹¶ä¿®å¤è®¾å¤‡è®¾ç½®"""
        if self.args.device == 'auto':
            if torch.cuda.is_available() and torch.cuda.device_count() > 0:
                # æµ‹è¯•CUDAæ˜¯å¦çœŸçš„å¯ç”¨
                try:
                    test_tensor = torch.tensor([1.0]).cuda()
                    self.args.device = 'cuda:0'
                    print(f"è‡ªåŠ¨é€‰æ‹©è®¾å¤‡: cuda:0")
                except Exception as e:
                    print(f"CUDAä¸å¯ç”¨ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°CPU: {e}")
                    self.args.device = 'cpu'
            else:
                print("æœªæ£€æµ‹åˆ°å¯ç”¨çš„CUDAè®¾å¤‡ï¼Œä½¿ç”¨CPU")
                self.args.device = 'cpu'
        
        # éªŒè¯è®¾å¤‡
        if 'cuda' in self.args.device:
            try:
                test_tensor = torch.tensor([1.0]).to(self.args.device)
                print(f"è®¾å¤‡éªŒè¯æˆåŠŸ: {self.args.device}")
            except Exception as e:
                print(f"è®¾å¤‡ {self.args.device} ä¸å¯ç”¨ï¼Œåˆ‡æ¢åˆ°CPU: {e}")
                self.args.device = 'cpu'

    def monitor_resources(self):
        """èµ„æºç›‘æ§å‡½æ•°"""
        # åˆå§‹åŒ–GPUç›‘æ§
        if torch.cuda.is_available() and NVML_AVAILABLE:
            try:
                pynvml.nvmlInit()
                self.nvml_handle = pynvml.nvmlDeviceGetHandleByIndex(0)
                if self.args.verbose:
                    print("GPUç›‘æ§å·²åˆå§‹åŒ–")
            except Exception as e:
                if self.args.verbose:
                    print(f"GPUç›‘æ§åˆå§‹åŒ–å¤±è´¥: {e}")
                self.nvml_handle = None
        
        # è·å–å½“å‰è¿›ç¨‹
        current_process = psutil.Process()
        
        while self.monitoring:
            try:
                # CPUä½¿ç”¨ç‡
                self.cpu_percentages.append(current_process.cpu_percent(interval=0.1))
                
                # å†…å­˜ä½¿ç”¨ç‡
                memory = psutil.virtual_memory()
                self.memory_usages.append(memory.percent)
                
                # GPUä½¿ç”¨ç‡
                if torch.cuda.is_available():
                    # GPUå†…å­˜ä½¿ç”¨ç‡
                    gpu_mem_alloc = torch.cuda.memory_allocated(0)
                    gpu_mem_total = torch.cuda.get_device_properties(0).total_memory
                    gpu_mem_percent = (gpu_mem_alloc / gpu_mem_total) * 100
                    self.gpu_mem_usages.append(gpu_mem_percent)
                    
                    # GPUåˆ©ç”¨ç‡
                    if self.nvml_handle is not None:
                        try:
                            utilization = pynvml.nvmlDeviceGetUtilizationRates(self.nvml_handle)
                            self.gpu_util_usages.append(utilization.gpu)
                        except Exception:
                            last_value = self.gpu_util_usages[-1] if self.gpu_util_usages else 0
                            self.gpu_util_usages.append(last_value)
                
                time.sleep(0.1)
                
            except Exception as e:
                if self.args.verbose:
                    print(f"ç›‘æ§é”™è¯¯: {e}")
                break
        
        # æ¸…ç†
        if torch.cuda.is_available() and self.nvml_handle is not None and NVML_AVAILABLE:
            try:
                pynvml.nvmlShutdown()
            except:
                pass
    
    def run_benchmark(self):
        """è¿è¡ŒåŸºå‡†æµ‹è¯•"""
        print("å¼€å§‹åŸºå‡†æµ‹è¯•...")
        
        # æ£€æŸ¥å¹¶ä¿®å¤è®¾å¤‡è®¾ç½®
        self.check_and_fix_device()
        
        # å¯åŠ¨èµ„æºç›‘æ§çº¿ç¨‹
        self.monitor_thread = threading.Thread(target=self.monitor_resources)
        self.monitor_thread.daemon = True
        self.monitor_thread.start()
        
        # éªŒè¯è¾“å…¥æº
        source = self.validate_source()
        
        # è®¾ç½®æ¨ç†å‚æ•°
        inference_params = {
            'stream': True,
            'batch': self.args.batch_size,
            'device': self.args.device,
            'verbose': False  # é¿å…å¹²æ‰°è¾“å‡º
        }
        
        if self.args.imgsz:
            inference_params['imgsz'] = self.args.imgsz
        
        if self.args.verbose:
            print(f"æ¨ç†å‚æ•°: {inference_params}")
            print(f"CPUé…ç½®: {self.cpu_count}æ ¸å¿ƒ/{self.cpu_logical_count}çº¿ç¨‹ (æœ€å¤§CPUä½¿ç”¨ç‡: {self.cpu_logical_count * 100}%)")
        
        self.start_time = time.time()
        
        # ç”¨äºè®¡ç®—ç¬æ—¶FPS
        last_time = self.start_time
        last_frame_count = 0
        
        try:
            # è¿è¡Œæ¨ç†
            results = self.model(source, **inference_params)
            
            # å¤„ç†ç»“æœ
            for result in results:
                # æ£€æŸ¥å¸§æ•°é™åˆ¶ï¼ˆå¦‚æœä¸æƒ³é™åˆ¶å¸§æ•°ï¼Œå¯ä»¥æ³¨é‡Šæ‰ä¸‹é¢å‡ è¡Œï¼‰
                if self.args.max_frames and self.total_frames >= self.args.max_frames:
                    if self.args.verbose:
                        print(f"è¾¾åˆ°æœ€å¤§å¸§æ•°é™åˆ¶: {self.args.max_frames}")
                    break
                
                # æ”¶é›†æŒ‡æ ‡
                speed_data = result.speed
                self.preprocess_times.append(speed_data.get("preprocess", 0))
                self.inference_times.append(speed_data.get("inference", 0))
                self.postprocess_times.append(speed_data.get("postprocess", 0))
                self.total_frames += 1
                
                # æ˜¾ç¤ºæ£€æµ‹ç»“æœï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if self.args.show_detections:
                    self.print_detection_results(result, self.total_frames)
                
                # æ˜¾ç¤ºè¿›åº¦ï¼ˆæ˜¾ç¤ºç´¯ç§¯å¹³å‡FPSå’Œç¬æ—¶FPSï¼‰
                if (self.args.show_progress or self.args.verbose) and self.total_frames % 10 == 0:
                    current_time = time.time()
                    
                    # ç´¯ç§¯å¹³å‡FPS
                    avg_fps = self.total_frames / (current_time - self.start_time)
                    
                    # ç¬æ—¶FPSï¼ˆæœ€è¿‘10å¸§çš„å¹³å‡ï¼‰
                    instant_fps = 10 / (current_time - last_time) if current_time > last_time else 0
                    
                    print(f"å·²å¤„ç† {self.total_frames} å¸§ | å¹³å‡FPS: {avg_fps:.2f} | ç¬æ—¶FPS: {instant_fps:.2f}")
                    
                    last_time = current_time
                    last_frame_count = self.total_frames
                
        except KeyboardInterrupt:
            print("\næµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        except Exception as e:
            print(f"æ¨ç†è¿‡ç¨‹å‡ºé”™: {e}")
            return False
        
        # åœæ­¢ç›‘æ§
        self.monitoring = False
        if self.monitor_thread and self.monitor_thread.is_alive():
            self.monitor_thread.join(timeout=1.0)
        
        if self.total_frames == 0:
            print("æ²¡æœ‰å¤„ç†ä»»ä½•å¸§")
            return False
        
        print(f"æµ‹è¯•å®Œæˆï¼Œå…±å¤„ç† {self.total_frames} å¸§")
        return True
    
    def calculate_statistics(self):
        """è®¡ç®—ç»Ÿè®¡æ•°æ®"""
        if not self.preprocess_times:
            return None
        
        end_time = time.time()
        total_time = end_time - self.start_time
        throughput = self.total_frames / total_time if total_time > 0 else 0
        
        total_per_frame = [p + i + pp for p, i, pp in zip(
            self.preprocess_times, self.inference_times, self.postprocess_times)]
        
        stats = {
            'summary': {
                'total_frames': self.total_frames,
                'total_time': total_time,
                'throughput': throughput,
                'avg_frame_time': np.mean(total_per_frame)
            },
            'timing': {
                'preprocess': {
                    'min': np.min(self.preprocess_times),
                    'max': np.max(self.preprocess_times),
                    'avg': np.mean(self.preprocess_times),
                    'std': np.std(self.preprocess_times)
                },
                'inference': {
                    'min': np.min(self.inference_times),
                    'max': np.max(self.inference_times),
                    'avg': np.mean(self.inference_times),
                    'std': np.std(self.inference_times)
                },
                'postprocess': {
                    'min': np.min(self.postprocess_times),
                    'max': np.max(self.postprocess_times),
                    'avg': np.mean(self.postprocess_times),
                    'std': np.std(self.postprocess_times)
                },
                'total_per_frame': {
                    'min': np.min(total_per_frame),
                    'max': np.max(total_per_frame),
                    'avg': np.mean(total_per_frame),
                    'std': np.std(total_per_frame)
                }
            },
            'resources': {
                'cpu': {
                    'min': np.min(self.cpu_percentages) if self.cpu_percentages else 0,
                    'max': np.max(self.cpu_percentages) if self.cpu_percentages else 0,
                    'avg': np.mean(self.cpu_percentages) if self.cpu_percentages else 0
                },
                'memory': {
                    'min': np.min(self.memory_usages) if self.memory_usages else 0,
                    'max': np.max(self.memory_usages) if self.memory_usages else 0,
                    'avg': np.mean(self.memory_usages) if self.memory_usages else 0
                }
            },
            'system_info': {
                'hostname': socket.gethostname(),
                'model': self.args.model,
                'source': self.args.source,
                'device': self.args.device,
                'batch_size': self.args.batch_size,
                'torch_version': torch.__version__,
                'cuda_available': torch.cuda.is_available(),
                'device_name': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU',
                'cpu_cores': self.cpu_count,
                'cpu_threads': self.cpu_logical_count,
                'cpu_max_usage': f"{self.cpu_logical_count * 100}%"
            }
        }
        
        # æ·»åŠ GPUç»Ÿè®¡
        if torch.cuda.is_available() and self.gpu_mem_usages and self.gpu_util_usages:
            stats['resources']['gpu'] = {
                'memory': {
                    'min': np.min(self.gpu_mem_usages),
                    'max': np.max(self.gpu_mem_usages),
                    'avg': np.mean(self.gpu_mem_usages)
                },
                'utilization': {
                    'min': np.min(self.gpu_util_usages),
                    'max': np.max(self.gpu_util_usages),
                    'avg': np.mean(self.gpu_util_usages)
                }
            }
        
        return stats
    
    def save_results(self, stats):
        """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
        if not stats:
            return
        
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        hostname = socket.gethostname()
        base_filename = f"{hostname}_{timestamp}"
        
        # ä¿å­˜TXTæ ¼å¼
        if 'txt' in self.args.output_format:
            txt_filename = f"{base_filename}.txt"
            self.save_txt_report(stats, txt_filename)
            print(f"TXTæŠ¥å‘Šä¿å­˜è‡³: {txt_filename}")
        
        # ä¿å­˜JSONæ ¼å¼
        if 'json' in self.args.output_format:
            json_filename = f"{base_filename}.json"
            with open(json_filename, 'w', encoding='utf-8') as f:
                json.dump(stats, f, indent=2, ensure_ascii=False)
            print(f"JSONæŠ¥å‘Šä¿å­˜è‡³: {json_filename}")
        
        # ä¿å­˜CSVæ ¼å¼
        if 'csv' in self.args.output_format:
            csv_filename = f"{base_filename}.csv"
            self.save_csv_report(stats, csv_filename)
            print(f"CSVæŠ¥å‘Šä¿å­˜è‡³: {csv_filename}")
    
    def save_txt_report(self, stats, filename):
        """ä¿å­˜æ–‡æœ¬æ ¼å¼æŠ¥å‘Š"""
        with open(filename, 'w', encoding='utf-8') as f:
            f.write("===== YOLOæ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š =====\n")
            f.write(f"æµ‹è¯•æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"ä¸»æœºå: {stats['system_info']['hostname']}\n")
            f.write(f"æ¨¡å‹: {stats['system_info']['model']}\n")
            f.write(f"æ•°æ®æº: {stats['system_info']['source']}\n")
            f.write(f"è®¾å¤‡: {stats['system_info']['device']}\n")
            f.write(f"æ‰¹æ¬¡å¤§å°: {stats['system_info']['batch_size']}\n")
            f.write(f"PyTorchç‰ˆæœ¬: {stats['system_info']['torch_version']}\n")
            f.write(f"CUDAå¯ç”¨: {stats['system_info']['cuda_available']}\n")
            f.write(f"è®¾å¤‡åç§°: {stats['system_info']['device_name']}\n\n")
            
            # æ‘˜è¦
            f.write("===== æµ‹è¯•æ‘˜è¦ =====\n")
            f.write(f"æ€»å¸§æ•°: {stats['summary']['total_frames']}\n")
            f.write(f"æ€»æ—¶é—´: {stats['summary']['total_time']:.2f} ç§’\n")
            f.write(f"ååé‡: {stats['summary']['throughput']:.2f} FPS\n")
            f.write(f"å¹³å‡å¸§æ—¶é—´: {stats['summary']['avg_frame_time']:.2f} æ¯«ç§’\n\n")
            
            # æ—¶é—´è¯¦æƒ…
            f.write("===== æ—¶é—´åˆ†è§£ =====\n")
            for stage, data in stats['timing'].items():
                stage_name = {'preprocess': 'é¢„å¤„ç†', 'inference': 'æ¨ç†', 
                             'postprocess': 'åå¤„ç†', 'total_per_frame': 'æ€»è®¡æ¯å¸§'}
                f.write(f"{stage_name.get(stage, stage)}:\n")
                f.write(f"  æœ€å°å€¼: {data['min']:.2f} æ¯«ç§’\n")
                f.write(f"  æœ€å¤§å€¼: {data['max']:.2f} æ¯«ç§’\n")
                f.write(f"  å¹³å‡å€¼: {data['avg']:.2f} æ¯«ç§’\n")
                f.write(f"  æ ‡å‡†å·®: {data['std']:.2f} æ¯«ç§’\n\n")
            
            # èµ„æºä½¿ç”¨
            f.write("===== èµ„æºä½¿ç”¨æƒ…å†µ =====\n")
            f.write(f"CPUä½¿ç”¨ç‡:\n")
            f.write(f"  æœ€å°å€¼: {stats['resources']['cpu']['min']:.1f}%\n")
            f.write(f"  æœ€å¤§å€¼: {stats['resources']['cpu']['max']:.1f}%\n")
            f.write(f"  å¹³å‡å€¼: {stats['resources']['cpu']['avg']:.1f}%\n\n")
            
            f.write(f"å†…å­˜ä½¿ç”¨ç‡:\n")
            f.write(f"  æœ€å°å€¼: {stats['resources']['memory']['min']:.1f}%\n")
            f.write(f"  æœ€å¤§å€¼: {stats['resources']['memory']['max']:.1f}%\n")
            f.write(f"  å¹³å‡å€¼: {stats['resources']['memory']['avg']:.1f}%\n\n")
            
            if 'gpu' in stats['resources']:
                f.write(f"GPUæ˜¾å­˜ä½¿ç”¨ç‡:\n")
                f.write(f"  æœ€å°å€¼: {stats['resources']['gpu']['memory']['min']:.1f}%\n")
                f.write(f"  æœ€å¤§å€¼: {stats['resources']['gpu']['memory']['max']:.1f}%\n")
                f.write(f"  å¹³å‡å€¼: {stats['resources']['gpu']['memory']['avg']:.1f}%\n\n")
                
                f.write(f"GPUåˆ©ç”¨ç‡:\n")
                f.write(f"  æœ€å°å€¼: {stats['resources']['gpu']['utilization']['min']:.1f}%\n")
                f.write(f"  æœ€å¤§å€¼: {stats['resources']['gpu']['utilization']['max']:.1f}%\n")
                f.write(f"  å¹³å‡å€¼: {stats['resources']['gpu']['utilization']['avg']:.1f}%\n\n")
    
    def save_csv_report(self, stats, filename):
        """ä¿å­˜CSVæ ¼å¼æŠ¥å‘Š"""
        with open(filename, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.writer(f)
            
            # è¡¨å¤´
            writer.writerow(['æŒ‡æ ‡', 'æ•°å€¼', 'å•ä½'])
            
            # æ‘˜è¦æ•°æ®
            writer.writerow(['æ€»å¸§æ•°', stats['summary']['total_frames'], 'å¸§'])
            writer.writerow(['æ€»æ—¶é—´', f"{stats['summary']['total_time']:.2f}", 'ç§’'])
            writer.writerow(['ååé‡', f"{stats['summary']['throughput']:.2f}", 'FPS'])
            writer.writerow(['å¹³å‡å¸§æ—¶é—´', f"{stats['summary']['avg_frame_time']:.2f}", 'æ¯«ç§’'])
            
            # æ¨ç†æ—¶é—´
            writer.writerow(['æ¨ç†æœ€å°æ—¶é—´', f"{stats['timing']['inference']['min']:.2f}", 'æ¯«ç§’'])
            writer.writerow(['æ¨ç†æœ€å¤§æ—¶é—´', f"{stats['timing']['inference']['max']:.2f}", 'æ¯«ç§’'])
            writer.writerow(['æ¨ç†å¹³å‡æ—¶é—´', f"{stats['timing']['inference']['avg']:.2f}", 'æ¯«ç§’'])
            
            # èµ„æºä½¿ç”¨
            writer.writerow(['CPUå¹³å‡ä½¿ç”¨ç‡', f"{stats['resources']['cpu']['avg']:.1f}", '%'])
            writer.writerow(['å†…å­˜å¹³å‡ä½¿ç”¨ç‡', f"{stats['resources']['memory']['avg']:.1f}", '%'])
            
            if 'gpu' in stats['resources']:
                writer.writerow(['GPUå¹³å‡åˆ©ç”¨ç‡', f"{stats['resources']['gpu']['utilization']['avg']:.1f}", '%'])
                writer.writerow(['GPUå¹³å‡æ˜¾å­˜ä½¿ç”¨', f"{stats['resources']['gpu']['memory']['avg']:.1f}", '%'])
    
    def print_summary(self, stats):
        """æ‰“å°è¯¦ç»†æ‘˜è¦åˆ°æ§åˆ¶å°"""
        if not stats:
            return
        
        print("\n" + "="*60)
        print("BENCHMARK SUMMARY")
        print("="*60)
        print(f"Total frames processed: {stats['summary']['total_frames']}")
        print(f"Total time elapsed: {stats['summary']['total_time']:.2f} seconds")
        print(f"Throughput: {stats['summary']['throughput']:.2f} frames per second")
        
        # è¯¦ç»†æ—¶é—´ç»Ÿè®¡
        print("\n" + "="*60)
        print("DETAILED METRICS")
        print("="*60)
        print("Preprocess time (ms):")
        print(f"  Min: {stats['timing']['preprocess']['min']:.2f}")
        print(f"  Max: {stats['timing']['preprocess']['max']:.2f}")
        print(f"  Avg: {stats['timing']['preprocess']['avg']:.2f}")
        
        print("\nInference time (ms):")
        print(f"  Min: {stats['timing']['inference']['min']:.2f}")
        print(f"  Max: {stats['timing']['inference']['max']:.2f}")
        print(f"  Avg: {stats['timing']['inference']['avg']:.2f}")
        
        print("\nPostprocess time (ms):")
        print(f"  Min: {stats['timing']['postprocess']['min']:.2f}")
        print(f"  Max: {stats['timing']['postprocess']['max']:.2f}")
        print(f"  Avg: {stats['timing']['postprocess']['avg']:.2f}")
        
        print("\nTotal processing time per frame (ms):")
        print(f"  Min: {stats['timing']['total_per_frame']['min']:.2f}")
        print(f"  Max: {stats['timing']['total_per_frame']['max']:.2f}")
        print(f"  Avg: {stats['timing']['total_per_frame']['avg']:.2f}")
        
        # èµ„æºä½¿ç”¨ç»Ÿè®¡
        print("\n" + "="*60)
        print("RESOURCE UTILIZATION")
        print("="*60)
        
        # CPUä½¿ç”¨ç‡è§£é‡Š
        cpu_avg = stats['resources']['cpu']['avg']
        cpu_cores = stats['system_info']['cpu_cores']
        cpu_threads = stats['system_info']['cpu_threads']
        cpu_utilization_percent = (cpu_avg / (cpu_threads * 100)) * 100
        
        print("CPU Usage:")
        print(f"  Min: {stats['resources']['cpu']['min']:.2f}%")
        print(f"  Max: {stats['resources']['cpu']['max']:.2f}%")
        print(f"  Avg: {stats['resources']['cpu']['avg']:.2f}%")
        print(f"  CPU Info: {cpu_cores}æ ¸å¿ƒ/{cpu_threads}çº¿ç¨‹ (æœ€å¤§{cpu_threads * 100}%)")
        print(f"  CPUåˆ©ç”¨ç‡: {cpu_utilization_percent:.1f}% çš„æ€»CPUèƒ½åŠ›")
        
        if cpu_avg > 100:
            print(f"  è¯´æ˜: ä½¿ç”¨äº†{cpu_avg/100:.1f}ä¸ªCPUæ ¸å¿ƒï¼Œè¿™æ˜¯æ­£å¸¸çš„å¤šæ ¸å¹¶è¡Œ")
        
        print("\nMemory Usage (%):")
        print(f"  Min: {stats['resources']['memory']['min']:.2f}")
        print(f"  Max: {stats['resources']['memory']['max']:.2f}")
        print(f"  Avg: {stats['resources']['memory']['avg']:.2f}")
        
        if 'gpu' in stats['resources']:
            print("\nGPU Memory Usage (%):")
            print(f"  Min: {stats['resources']['gpu']['memory']['min']:.2f}")
            print(f"  Max: {stats['resources']['gpu']['memory']['max']:.2f}")
            print(f"  Avg: {stats['resources']['gpu']['memory']['avg']:.2f}")
            
            print("\nGPU Utilization (%):")
            print(f"  Min: {stats['resources']['gpu']['utilization']['min']:.2f}")
            print(f"  Max: {stats['resources']['gpu']['utilization']['max']:.2f}")
            print(f"  Avg: {stats['resources']['gpu']['utilization']['avg']:.2f}")
            print(f"  Device: {stats['system_info']['device_name']}")
        else:
            print("\nGPU: Not available")
        
        # æ€§èƒ½è¯„çº§
        fps = stats['summary']['throughput']
        if fps > 30:
            rating = "Excellent ğŸŸ¢"
        elif fps > 15:
            rating = "Good ğŸŸ¡"
        elif fps > 5:
            rating = "Fair ğŸŸ "
        else:
            rating = "Slow ğŸ”´"
        
        print(f"\nPerformance Rating: {rating}")
        print(f"Model: {stats['system_info']['model']}")
        print(f"Device: {stats['system_info']['device']}")
        print(f"Batch Size: {stats['system_info']['batch_size']}")


def main():
    """ä¸»å‡½æ•°"""
    print("YOLOv8 æ€§èƒ½åŸºå‡†æµ‹è¯•å·¥å…· - æ”¹è¿›ç‰ˆ")
    print("-" * 50)
    
    # è§£æå‚æ•°
    args = parse_arguments()
    
    # åˆ›å»ºåŸºå‡†æµ‹è¯•å·¥å…·
    benchmark = BenchmarkTool(args)
    
    # åŠ è½½æ¨¡å‹
    benchmark.load_model()
    
    # è¿è¡Œæµ‹è¯•
    success = benchmark.run_benchmark()
    
    if success:
        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        stats = benchmark.calculate_statistics()
        
        # æ‰“å°æ‘˜è¦
        benchmark.print_summary(stats)
        
        # ä¿å­˜ç»“æœ
        benchmark.save_results(stats)
        
        print("\næµ‹è¯•å®Œæˆ!")
    else:
        print("æµ‹è¯•å¤±è´¥")
        sys.exit(1)


if __name__ == "__main__":
    main()