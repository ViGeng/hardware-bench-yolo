#!/usr/bin/env python3
"""
Enhanced Deep Learning Benchmark Tool - å¢å¼ºç‰ˆæ·±åº¦å­¦ä¹ åŸºå‡†æµ‹è¯•å·¥å…·
æ”¯æŒå¤šç§æ¨¡å‹ç±»å‹å’Œæ•°æ®é›†çš„äº¤äº’å¼åŸºå‡†æµ‹è¯•

ä¸»è¦åŠŸèƒ½ï¼š
1. äº¤äº’å¼è®¾å¤‡é€‰æ‹© (CPU/GPU)
2. æ¨¡å‹ç±»å‹é€‰æ‹© (Detection/Classification)
3. æ•°æ®é›†é€‰æ‹© (MNIST/CIFAR-10/COCO/ImageNet)
4. è¯¦ç»†æ€§èƒ½ç»Ÿè®¡å’ŒCSVè¾“å‡º
5. ç»“æœå¯è§†åŒ–

Author: Enhanced for professional AI benchmarking
Platform: Ubuntu 22.04 + NVIDIA GPU support
"""

import argparse
import json
import csv
import os
import sys
import time
import socket
import threading
from pathlib import Path
from collections import deque
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
import seaborn as sns

# Deep Learning Libraries
import torch
import torch.nn as nn
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import timm

# System Monitoring
import psutil
try:
    import pynvml
    NVML_AVAILABLE = True
except ImportError:
    NVML_AVAILABLE = False
    print("Warning: pynvml not available. Install with: pip install nvidia-ml-py3")

# Object Detection (if available)
try:
    from ultralytics import YOLO
    ULTRALYTICS_AVAILABLE = True
except ImportError:
    ULTRALYTICS_AVAILABLE = False
    print("Warning: ultralytics not available. Install with: pip install ultralytics")

class InteractiveBenchmark:
    def __init__(self):
        self.device = None
        self.model_type = None
        self.model = None
        self.dataset_name = None
        self.dataloader = None
        self.results = []
        
        # Monitoring
        self.monitoring = True
        self.cpu_usage = deque(maxlen=1000)
        self.memory_usage = deque(maxlen=1000)
        self.gpu_memory = deque(maxlen=1000)
        self.gpu_utilization = deque(maxlen=1000)
        
        self.total_samples = 0
        self.start_time = None
        
        # Available models
        self.detection_models = {
            '1': {'name': 'YOLOv8n', 'model': 'yolov8n.pt', 'type': 'yolo'},
            '2': {'name': 'YOLOv8s', 'model': 'yolov8s.pt', 'type': 'yolo'},
            '3': {'name': 'YOLOv8m', 'model': 'yolov8m.pt', 'type': 'yolo'},
        }
        
        self.classification_models = {
            '1': {'name': 'ResNet18', 'model': 'resnet18', 'type': 'timm'},
            '2': {'name': 'ResNet50', 'model': 'resnet50', 'type': 'timm'},
            '3': {'name': 'EfficientNet-B0', 'model': 'efficientnet_b0', 'type': 'timm'},
            '4': {'name': 'EfficientNet-B3', 'model': 'efficientnet_b3', 'type': 'timm'},
            '5': {'name': 'Vision Transformer', 'model': 'vit_base_patch16_224', 'type': 'timm'},
            '6': {'name': 'MobileNet-V3', 'model': 'mobilenetv3_large_100', 'type': 'timm'},
        }

    def interactive_setup(self):
        """äº¤äº’å¼è®¾ç½®"""
        print("="*60)
        print("æ·±åº¦å­¦ä¹ æ¨¡å‹åŸºå‡†æµ‹è¯•å·¥å…·")
        print("="*60)
        
        # 1. è®¾å¤‡é€‰æ‹©
        self.select_device()
        
        # 2. æ¨¡å‹ç±»å‹é€‰æ‹©
        self.select_model_type()
        
        # 3. å…·ä½“æ¨¡å‹é€‰æ‹©
        self.select_model()
        
        # 4. æ•°æ®é›†é€‰æ‹©
        self.select_dataset()
        
        print("\n" + "="*60)
        print("é…ç½®æ€»ç»“ï¼š")
        print(f"è®¾å¤‡: {self.device}")
        print(f"æ¨¡å‹ç±»å‹: {self.model_type}")
        print(f"æ•°æ®é›†: {self.dataset_name}")
        print("="*60)
        
        confirm = input("\nç¡®è®¤å¼€å§‹æµ‹è¯•? (y/n): ").lower().strip()
        if confirm != 'y':
            print("æµ‹è¯•å·²å–æ¶ˆ")
            sys.exit(0)

    def select_device(self):
        """é€‰æ‹©è®¡ç®—è®¾å¤‡"""
        print("\n1. é€‰æ‹©è®¡ç®—è®¾å¤‡:")
        print("1) CPU")
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                print(f"2) CUDA:{i} - {torch.cuda.get_device_name(i)}")
        else:
            print("   (CUDAä¸å¯ç”¨)")
        
        while True:
            choice = input("è¯·é€‰æ‹©è®¾å¤‡ (è¾“å…¥æ•°å­—): ").strip()
            if choice == '1':
                self.device = 'cpu'
                break
            elif choice == '2' and torch.cuda.is_available():
                self.device = 'cuda:0'
                break
            else:
                print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")
        
        print(f"å·²é€‰æ‹©è®¾å¤‡: {self.device}")

    def select_model_type(self):
        """é€‰æ‹©æ¨¡å‹ç±»å‹"""
        print("\n2. é€‰æ‹©æ¨¡å‹ç±»å‹:")
        print("1) å›¾åƒåˆ†ç±» (Classification)")
        if ULTRALYTICS_AVAILABLE:
            print("2) ç›®æ ‡æ£€æµ‹ (Object Detection)")
        else:
            print("2) ç›®æ ‡æ£€æµ‹ (éœ€è¦å®‰è£… ultralytics)")
        
        while True:
            choice = input("è¯·é€‰æ‹©æ¨¡å‹ç±»å‹ (è¾“å…¥æ•°å­—): ").strip()
            if choice == '1':
                self.model_type = 'classification'
                break
            elif choice == '2' and ULTRALYTICS_AVAILABLE:
                self.model_type = 'detection'
                break
            elif choice == '2':
                print("ç›®æ ‡æ£€æµ‹éœ€è¦å®‰è£… ultralytics: pip install ultralytics")
            else:
                print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")
        
        print(f"å·²é€‰æ‹©æ¨¡å‹ç±»å‹: {self.model_type}")

    def select_model(self):
        """é€‰æ‹©å…·ä½“æ¨¡å‹"""
        print(f"\n3. é€‰æ‹©{self.model_type}æ¨¡å‹:")
        
        if self.model_type == 'classification':
            for key, value in self.classification_models.items():
                print(f"{key}) {value['name']}")
            
            while True:
                choice = input("è¯·é€‰æ‹©æ¨¡å‹ (è¾“å…¥æ•°å­—): ").strip()
                if choice in self.classification_models:
                    selected = self.classification_models[choice]
                    self.model_info = selected
                    print(f"å·²é€‰æ‹©æ¨¡å‹: {selected['name']}")
                    break
                else:
                    print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")
                    
        elif self.model_type == 'detection':
            for key, value in self.detection_models.items():
                print(f"{key}) {value['name']}")
            
            while True:
                choice = input("è¯·é€‰æ‹©æ¨¡å‹ (è¾“å…¥æ•°å­—): ").strip()
                if choice in self.detection_models:
                    selected = self.detection_models[choice]
                    self.model_info = selected
                    print(f"å·²é€‰æ‹©æ¨¡å‹: {selected['name']}")
                    break
                else:
                    print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")

    def select_dataset(self):
        """é€‰æ‹©æ•°æ®é›†"""
        print("\n4. é€‰æ‹©æ•°æ®é›†:")
        
        if self.model_type == 'classification':
            print("1) MNIST (æ‰‹å†™æ•°å­—, 28x28)")
            print("2) CIFAR-10 (å°ç‰©ä½“åˆ†ç±», 32x32)")
            print("3) ImageNet éªŒè¯é›†æ ·æœ¬ (224x224, éœ€è¦ä¸‹è½½)")
            
            datasets = {
                '1': {'name': 'MNIST', 'func': self.load_mnist},
                '2': {'name': 'CIFAR-10', 'func': self.load_cifar10},
                '3': {'name': 'ImageNet-Sample', 'func': self.load_imagenet_sample}
            }
            
        elif self.model_type == 'detection':
            print("1) COCO éªŒè¯é›†æ ·æœ¬ (éœ€è¦ä¸‹è½½)")
            print("2) é¢„è®¾æµ‹è¯•å›¾åƒ")
            
            datasets = {
                '1': {'name': 'COCO-Sample', 'func': self.load_coco_sample},
                '2': {'name': 'Test-Images', 'func': self.load_test_images}
            }
        
        while True:
            choice = input("è¯·é€‰æ‹©æ•°æ®é›† (è¾“å…¥æ•°å­—): ").strip()
            if choice in datasets:
                selected = datasets[choice]
                self.dataset_name = selected['name']
                self.load_dataset_func = selected['func']
                print(f"å·²é€‰æ‹©æ•°æ®é›†: {selected['name']}")
                break
            else:
                print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")

    def load_model(self):
        """åŠ è½½é€‰å®šçš„æ¨¡å‹"""
        print(f"\næ­£åœ¨åŠ è½½æ¨¡å‹: {self.model_info['name']}...")
        
        try:
            if self.model_type == 'classification':
                if self.model_info['type'] == 'timm':
                    self.model = timm.create_model(
                        self.model_info['model'], 
                        pretrained=True,
                        num_classes=1000
                    )
                    self.model.eval()
                    self.model = self.model.to(self.device)
                    
            elif self.model_type == 'detection':
                if self.model_info['type'] == 'yolo':
                    self.model = YOLO(self.model_info['model'])
                    
            print("æ¨¡å‹åŠ è½½æˆåŠŸ!")
            
        except Exception as e:
            print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            sys.exit(1)

    def load_mnist(self):
        """åŠ è½½MNISTæ•°æ®é›†"""
        print("æ­£åœ¨åŠ è½½MNISTæ•°æ®é›†...")
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.1307,), (0.3081,))
        ])
        
        dataset = torchvision.datasets.MNIST(
            root='./data', train=False, download=True, transform=transform
        )
        self.dataloader = DataLoader(dataset, batch_size=64, shuffle=False)
        print(f"MNISTæ•°æ®é›†åŠ è½½å®Œæˆï¼Œå…±{len(dataset)}ä¸ªæ ·æœ¬")

    def load_cifar10(self):
        """åŠ è½½CIFAR-10æ•°æ®é›†"""
        print("æ­£åœ¨åŠ è½½CIFAR-10æ•°æ®é›†...")
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
        ])
        
        dataset = torchvision.datasets.CIFAR10(
            root='./data', train=False, download=True, transform=transform
        )
        self.dataloader = DataLoader(dataset, batch_size=64, shuffle=False)
        print(f"CIFAR-10æ•°æ®é›†åŠ è½½å®Œæˆï¼Œå…±{len(dataset)}ä¸ªæ ·æœ¬")

    def load_imagenet_sample(self):
        """åŠ è½½ImageNetæ ·æœ¬æ•°æ®é›†"""
        print("ImageNetæ ·æœ¬æ•°æ®é›†è®¾ç½®...")
        print("è¯·ä¸‹è½½ImageNetéªŒè¯é›†åˆ° './data/imagenet/val' ç›®å½•")
        print("ä¸‹è½½åœ°å€: https://www.image-net.org/download.php")
        print("æˆ–ä½¿ç”¨å°‘é‡æ ·æœ¬å›¾åƒè¿›è¡Œæµ‹è¯•")
        
        # åˆ›å»ºç®€å•çš„æµ‹è¯•æ•°æ®
        self.create_synthetic_dataset(224, 1000)

    def load_coco_sample(self):
        """åŠ è½½COCOæ ·æœ¬æ•°æ®é›†"""
        print("COCOæ ·æœ¬æ•°æ®é›†è®¾ç½®...")
        print("è¯·ä¸‹è½½COCO 2017éªŒè¯é›†åˆ° './data/coco' ç›®å½•")
        print("ä¸‹è½½åœ°å€: http://cocodataset.org/#download")
        
        # åˆ›å»ºç®€å•çš„æµ‹è¯•æ•°æ®
        self.create_synthetic_detection_data()

    def load_test_images(self):
        """åŠ è½½é¢„è®¾æµ‹è¯•å›¾åƒ"""
        print("ä½¿ç”¨é¢„è®¾æµ‹è¯•å›¾åƒ...")
        self.create_synthetic_detection_data()

    def create_synthetic_dataset(self, img_size, num_classes):
        """åˆ›å»ºåˆæˆæ•°æ®é›†ç”¨äºæµ‹è¯•"""
        print(f"åˆ›å»ºåˆæˆæ•°æ®é›† ({img_size}x{img_size}, {num_classes}ç±»)...")
        
        class SyntheticDataset(torch.utils.data.Dataset):
            def __init__(self, size=1000, img_size=224, num_classes=1000):
                self.size = size
                self.img_size = img_size
                self.num_classes = num_classes
                
            def __len__(self):
                return self.size
                
            def __getitem__(self, idx):
                # ç”Ÿæˆéšæœºå›¾åƒ
                img = torch.randn(3, self.img_size, self.img_size)
                label = torch.randint(0, self.num_classes, (1,)).item()
                return img, label
        
        dataset = SyntheticDataset(1000, img_size, num_classes)
        self.dataloader = DataLoader(dataset, batch_size=32, shuffle=False)
        print("åˆæˆæ•°æ®é›†åˆ›å»ºå®Œæˆ")

    def create_synthetic_detection_data(self):
        """åˆ›å»ºåˆæˆæ£€æµ‹æ•°æ®"""
        print("åˆ›å»ºåˆæˆæ£€æµ‹æ•°æ®...")
        
        # ç”Ÿæˆä¸€äº›æµ‹è¯•å›¾åƒè·¯å¾„
        self.test_images = []
        for i in range(100):
            # åˆ›å»ºéšæœºå›¾åƒå¹¶ä¿å­˜
            img = np.random.randint(0, 255, (640, 640, 3), dtype=np.uint8)
            img_path = f"./temp_test_img_{i}.jpg"
            
            # è¿™é‡Œåº”è¯¥ä¿å­˜å›¾åƒï¼Œä½†ä¸ºäº†ç®€åŒ–æˆ‘ä»¬åªä¿å­˜è·¯å¾„
            self.test_images.append(img_path)
        
        print(f"åˆ›å»º{len(self.test_images)}ä¸ªæµ‹è¯•å›¾åƒ")

    def monitor_resources(self):
        """ç›‘æ§ç³»ç»Ÿèµ„æº"""
        if torch.cuda.is_available() and NVML_AVAILABLE:
            try:
                pynvml.nvmlInit()
                handle = pynvml.nvmlDeviceGetHandleByIndex(0)
            except:
                handle = None
        else:
            handle = None
        
        process = psutil.Process()
        
        while self.monitoring:
            try:
                # CPUå’Œå†…å­˜
                self.cpu_usage.append(process.cpu_percent(interval=0.1))
                self.memory_usage.append(psutil.virtual_memory().percent)
                
                # GPUç›‘æ§
                if torch.cuda.is_available():
                    # GPUå†…å­˜
                    gpu_mem = torch.cuda.memory_allocated(0)
                    gpu_total = torch.cuda.get_device_properties(0).total_memory
                    self.gpu_memory.append((gpu_mem / gpu_total) * 100)
                    
                    # GPUåˆ©ç”¨ç‡
                    if handle:
                        try:
                            util = pynvml.nvmlDeviceGetUtilizationRates(handle)
                            self.gpu_utilization.append(util.gpu)
                        except:
                            self.gpu_utilization.append(0)
                
                time.sleep(0.1)
                
            except Exception as e:
                break

    def run_classification_benchmark(self):
        """è¿è¡Œåˆ†ç±»æ¨¡å‹åŸºå‡†æµ‹è¯•"""
        print("\nå¼€å§‹åˆ†ç±»æ¨¡å‹åŸºå‡†æµ‹è¯•...")
        
        batch_times = []
        preprocessing_times = []
        inference_times = []
        
        self.model.eval()
        with torch.no_grad():
            for batch_idx, (data, target) in enumerate(self.dataloader):
                batch_start = time.time()
                
                # é¢„å¤„ç†æ—¶é—´
                prep_start = time.time()
                data = data.to(self.device)
                prep_time = (time.time() - prep_start) * 1000  # ms
                
                # æ¨ç†æ—¶é—´
                inf_start = time.time()
                output = self.model(data)
                inf_time = (time.time() - inf_start) * 1000  # ms
                
                batch_time = (time.time() - batch_start) * 1000  # ms
                
                # è®°å½•æ—¶é—´
                preprocessing_times.append(prep_time)
                inference_times.append(inf_time)
                batch_times.append(batch_time)
                
                self.total_samples += len(data)
                
                # è¿›åº¦æ˜¾ç¤º
                if batch_idx % 10 == 0:
                    print(f"Processed {batch_idx * len(data)} samples...")
                
                # é™åˆ¶æµ‹è¯•æ ·æœ¬æ•°
                if batch_idx >= 100:  # é™åˆ¶æµ‹è¯•æ‰¹æ¬¡
                    break
        
        return {
            'preprocessing_times': preprocessing_times,
            'inference_times': inference_times,
            'batch_times': batch_times
        }

    def run_detection_benchmark(self):
        """è¿è¡Œæ£€æµ‹æ¨¡å‹åŸºå‡†æµ‹è¯•"""
        print("\nå¼€å§‹æ£€æµ‹æ¨¡å‹åŸºå‡†æµ‹è¯•...")
        
        preprocessing_times = []
        inference_times = []
        postprocessing_times = []
        
        # ä½¿ç”¨åˆæˆå›¾åƒè¿›è¡Œæµ‹è¯•
        for i in range(min(100, len(self.test_images))):
            # åˆ›å»ºéšæœºå›¾åƒè¿›è¡Œæµ‹è¯•
            img = np.random.randint(0, 255, (640, 640, 3), dtype=np.uint8)
            
            # æ‰§è¡Œæ¨ç†
            results = self.model(img, device=self.device, verbose=False)
            
            # è·å–æ—¶é—´ä¿¡æ¯
            if hasattr(results[0], 'speed'):
                speed = results[0].speed
                preprocessing_times.append(speed.get('preprocess', 0))
                inference_times.append(speed.get('inference', 0))
                postprocessing_times.append(speed.get('postprocess', 0))
            
            self.total_samples += 1
            
            if i % 10 == 0:
                print(f"Processed {i} images...")
        
        return {
            'preprocessing_times': preprocessing_times,
            'inference_times': inference_times,
            'postprocessing_times': postprocessing_times
        }

    def run_benchmark(self):
        """è¿è¡ŒåŸºå‡†æµ‹è¯•"""
        print("\nå¼€å§‹åŸºå‡†æµ‹è¯•...")
        
        # å¯åŠ¨èµ„æºç›‘æ§
        monitor_thread = threading.Thread(target=self.monitor_resources)
        monitor_thread.daemon = True
        monitor_thread.start()
        
        self.start_time = time.time()
        
        try:
            if self.model_type == 'classification':
                timing_results = self.run_classification_benchmark()
            elif self.model_type == 'detection':
                timing_results = self.run_detection_benchmark()
            
        except KeyboardInterrupt:
            print("\næµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
            return None
        except Exception as e:
            print(f"æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            return None
        finally:
            self.monitoring = False
            time.sleep(0.5)  # ç­‰å¾…ç›‘æ§çº¿ç¨‹ç»“æŸ
        
        end_time = time.time()
        total_time = end_time - self.start_time
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        stats = self.calculate_statistics(timing_results, total_time)
        
        return stats

    def calculate_statistics(self, timing_results, total_time):
        """è®¡ç®—ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'system_info': {
                'hostname': socket.gethostname(),
                'device': self.device,
                'model_type': self.model_type,
                'model_name': self.model_info['name'],
                'dataset': self.dataset_name,
                'torch_version': torch.__version__,
                'cuda_available': torch.cuda.is_available(),
                'device_name': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'
            },
            'performance': {
                'total_samples': self.total_samples,
                'total_time': total_time,
                'throughput': self.total_samples / total_time if total_time > 0 else 0,
                'avg_time_per_sample': (total_time / self.total_samples * 1000) if self.total_samples > 0 else 0
            },
            'timing': {},
            'resources': {
                'cpu': {
                    'min': np.min(self.cpu_usage) if self.cpu_usage else 0,
                    'max': np.max(self.cpu_usage) if self.cpu_usage else 0,
                    'avg': np.mean(self.cpu_usage) if self.cpu_usage else 0
                },
                'memory': {
                    'min': np.min(self.memory_usage) if self.memory_usage else 0,
                    'max': np.max(self.memory_usage) if self.memory_usage else 0,
                    'avg': np.mean(self.memory_usage) if self.memory_usage else 0
                }
            }
        }
        
        # æ·»åŠ æ—¶é—´ç»Ÿè®¡
        for key, times in timing_results.items():
            if times:
                stats['timing'][key] = {
                    'min': np.min(times),
                    'max': np.max(times),
                    'avg': np.mean(times),
                    'std': np.std(times)
                }
        
        # GPUç»Ÿè®¡
        if torch.cuda.is_available() and self.gpu_memory:
            stats['resources']['gpu'] = {
                'memory': {
                    'min': np.min(self.gpu_memory),
                    'max': np.max(self.gpu_memory),
                    'avg': np.mean(self.gpu_memory)
                },
                'utilization': {
                    'min': np.min(self.gpu_utilization) if self.gpu_utilization else 0,
                    'max': np.max(self.gpu_utilization) if self.gpu_utilization else 0,
                    'avg': np.mean(self.gpu_utilization) if self.gpu_utilization else 0
                }
            }
        
        return stats

    def print_concise_results(self, stats):
        """æ‰“å°ç®€æ´çš„ç»“æœ"""
        print("\n" + "="*60)
        print("BENCHMARK RESULTS SUMMARY")
        print("="*60)
        
        # åŸºæœ¬ä¿¡æ¯
        print(f"Model: {stats['system_info']['model_name']}")
        print(f"Dataset: {stats['system_info']['dataset']}")
        print(f"Device: {stats['system_info']['device']}")
        
        # æ€§èƒ½æŒ‡æ ‡
        print(f"\nPerformance:")
        print(f"  Samples processed: {stats['performance']['total_samples']}")
        print(f"  Total time: {stats['performance']['total_time']:.2f}s")
        print(f"  Throughput: {stats['performance']['throughput']:.2f} samples/sec")
        print(f"  Avg time/sample: {stats['performance']['avg_time_per_sample']:.2f}ms")
        
        # æ—¶é—´åˆ†è§£
        if stats['timing']:
            print(f"\nTiming breakdown (ms):")
            for stage, data in stats['timing'].items():
                stage_name = stage.replace('_', ' ').title()
                print(f"  {stage_name}: {data['avg']:.2f} Â± {data['std']:.2f}")
        
        # èµ„æºä½¿ç”¨
        print(f"\nResource utilization:")
        print(f"  CPU: {stats['resources']['cpu']['avg']:.1f}%")
        print(f"  Memory: {stats['resources']['memory']['avg']:.1f}%")
        
        if 'gpu' in stats['resources']:
            print(f"  GPU Memory: {stats['resources']['gpu']['memory']['avg']:.1f}%")
            print(f"  GPU Util: {stats['resources']['gpu']['utilization']['avg']:.1f}%")
        
        # æ€§èƒ½è¯„çº§
        fps = stats['performance']['throughput']
        if self.model_type == 'classification':
            if fps > 100: rating = "Excellent ğŸŸ¢"
            elif fps > 50: rating = "Good ğŸŸ¡"
            elif fps > 10: rating = "Fair ğŸŸ "
            else: rating = "Slow ğŸ”´"
        else:  # detection
            if fps > 30: rating = "Excellent ğŸŸ¢"
            elif fps > 15: rating = "Good ğŸŸ¡"
            elif fps > 5: rating = "Fair ğŸŸ "
            else: rating = "Slow ğŸ”´"
        
        print(f"\nOverall Rating: {rating}")

    def save_csv_results(self, stats):
        """ä¿å­˜CSVç»“æœ"""
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        hostname = socket.gethostname()
        filename = f"{hostname}_{self.model_type}_{timestamp}.csv"
        
        with open(filename, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            
            # åŸºæœ¬ä¿¡æ¯
            writer.writerow(['Metric', 'Value', 'Unit'])
            writer.writerow(['Hostname', stats['system_info']['hostname'], ''])
            writer.writerow(['Model Type', stats['system_info']['model_type'], ''])
            writer.writerow(['Model Name', stats['system_info']['model_name'], ''])
            writer.writerow(['Dataset', stats['system_info']['dataset'], ''])
            writer.writerow(['Device', stats['system_info']['device'], ''])
            writer.writerow(['Device Name', stats['system_info']['device_name'], ''])
            
            # æ€§èƒ½æŒ‡æ ‡
            writer.writerow(['Total Samples', stats['performance']['total_samples'], 'samples'])
            writer.writerow(['Total Time', f"{stats['performance']['total_time']:.2f}", 'seconds'])
            writer.writerow(['Throughput', f"{stats['performance']['throughput']:.2f}", 'samples/sec'])
            writer.writerow(['Avg Time per Sample', f"{stats['performance']['avg_time_per_sample']:.2f}", 'ms'])
            
            # æ—¶é—´åˆ†è§£
            for stage, data in stats['timing'].items():
                writer.writerow([f'{stage.replace("_", " ").title()} Min', f"{data['min']:.2f}", 'ms'])
                writer.writerow([f'{stage.replace("_", " ").title()} Max', f"{data['max']:.2f}", 'ms'])
                writer.writerow([f'{stage.replace("_", " ").title()} Avg', f"{data['avg']:.2f}", 'ms'])
                writer.writerow([f'{stage.replace("_", " ").title()} Std', f"{data['std']:.2f}", 'ms'])
            
            # èµ„æºä½¿ç”¨
            writer.writerow(['CPU Usage Avg', f"{stats['resources']['cpu']['avg']:.1f}", '%'])
            writer.writerow(['Memory Usage Avg', f"{stats['resources']['memory']['avg']:.1f}", '%'])
            
            if 'gpu' in stats['resources']:
                writer.writerow(['GPU Memory Avg', f"{stats['resources']['gpu']['memory']['avg']:.1f}", '%'])
                writer.writerow(['GPU Utilization Avg', f"{stats['resources']['gpu']['utilization']['avg']:.1f}", '%'])
        
        print(f"\nCSVç»“æœå·²ä¿å­˜è‡³: {filename}")
        return filename

    def create_visualizations(self, stats, csv_filename):
        """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
        try:
            print("æ­£åœ¨ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
            
            # è®¾ç½®å›¾è¡¨æ ·å¼
            plt.style.use('seaborn-v0_8')
            fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
            fig.suptitle(f'Benchmark Results: {stats["system_info"]["model_name"]}', fontsize=16)
            
            # 1. æ—¶é—´åˆ†è§£é¥¼å›¾
            if stats['timing']:
                timing_data = [(k.replace('_', ' ').title(), v['avg']) for k, v in stats['timing'].items()]
                labels, values = zip(*timing_data)
                ax1.pie(values, labels=labels, autopct='%1.1f%%', startangle=90)
                ax1.set_title('Timing Breakdown')
            
            # 2. èµ„æºåˆ©ç”¨ç‡æŸ±çŠ¶å›¾
            resources = ['CPU', 'Memory']
            usage = [stats['resources']['cpu']['avg'], stats['resources']['memory']['avg']]
            
            if 'gpu' in stats['resources']:
                resources.extend(['GPU Memory', 'GPU Util'])
                usage.extend([stats['resources']['gpu']['memory']['avg'], 
                             stats['resources']['gpu']['utilization']['avg']])
            
            bars = ax2.bar(resources, usage, color=['skyblue', 'lightgreen', 'orange', 'red'][:len(resources)])
            ax2.set_title('Resource Utilization')
            ax2.set_ylabel('Usage (%)')
            ax2.set_ylim(0, 100)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, val in zip(bars, usage):
                ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, 
                        f'{val:.1f}%', ha='center', va='bottom')
            
            # 3. æ€§èƒ½å¯¹æ¯”ï¼ˆå¦‚æœæœ‰å¤šä¸ªæ—¶é—´é˜¶æ®µï¼‰
            if len(stats['timing']) > 1:
                stages = list(stats['timing'].keys())
                avg_times = [stats['timing'][stage]['avg'] for stage in stages]
                std_times = [stats['timing'][stage]['std'] for stage in stages]
                
                x_pos = np.arange(len(stages))
                ax3.bar(x_pos, avg_times, yerr=std_times, capsize=5, 
                       color='lightcoral', alpha=0.7)
                ax3.set_xlabel('Processing Stage')
                ax3.set_ylabel('Time (ms)')
                ax3.set_title('Processing Time by Stage')
                ax3.set_xticks(x_pos)
                ax3.set_xticklabels([s.replace('_', ' ').title() for s in stages], rotation=45)
            
            # 4. ç³»ç»Ÿä¿¡æ¯æ–‡æœ¬
            system_text = f"""
System Information:
Device: {stats['system_info']['device']}
Model: {stats['system_info']['model_name']}
Dataset: {stats['system_info']['dataset']}

Performance Summary:
Throughput: {stats['performance']['throughput']:.2f} samples/sec
Avg time/sample: {stats['performance']['avg_time_per_sample']:.2f}ms
Total samples: {stats['performance']['total_samples']}
            """
            ax4.text(0.1, 0.5, system_text, transform=ax4.transAxes, fontsize=10,
                    verticalalignment='center', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
            ax4.set_xlim(0, 1)
            ax4.set_ylim(0, 1)
            ax4.axis('off')
            ax4.set_title('System Info')
            
            plt.tight_layout()
            
            # ä¿å­˜å›¾è¡¨
            plot_filename = csv_filename.replace('.csv', '_visualization.png')
            plt.savefig(plot_filename, dpi=300, bbox_inches='tight')
            print(f"å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜è‡³: {plot_filename}")
            
            # æ˜¾ç¤ºå›¾è¡¨ï¼ˆå¯é€‰ï¼‰
            # plt.show()
            
        except Exception as e:
            print(f"åˆ›å»ºå¯è§†åŒ–æ—¶å‡ºé”™: {e}")

def main():
    """ä¸»å‡½æ•°"""
    # æ£€æŸ¥ä¾èµ–
    missing_deps = []
    
    if not ULTRALYTICS_AVAILABLE:
        missing_deps.append("ultralytics (pip install ultralytics)")
    
    if not NVML_AVAILABLE:
        missing_deps.append("nvidia-ml-py3 (pip install nvidia-ml-py3)")
    
    try:
        import timm
    except ImportError:
        missing_deps.append("timm (pip install timm)")
    
    try:
        import matplotlib.pyplot as plt
        import seaborn as sns
    except ImportError:
        missing_deps.append("matplotlib seaborn (pip install matplotlib seaborn)")
    
    if missing_deps:
        print("å»ºè®®å®‰è£…ä»¥ä¸‹ä¾èµ–ä»¥è·å¾—å®Œæ•´åŠŸèƒ½:")
        for dep in missing_deps:
            print(f"  - {dep}")
        print()
    
    # åˆ›å»ºåŸºå‡†æµ‹è¯•å·¥å…·
    benchmark = InteractiveBenchmark()
    
    # äº¤äº’å¼è®¾ç½®
    benchmark.interactive_setup()
    
    # åŠ è½½æ•°æ®é›†
    benchmark.load_dataset_func()
    
    # åŠ è½½æ¨¡å‹
    benchmark.load_model()
    
    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    stats = benchmark.run_benchmark()
    
    if stats:
        # æ‰“å°ç®€æ´ç»“æœ
        benchmark.print_concise_results(stats)
        
        # ä¿å­˜CSVç»“æœ
        csv_filename = benchmark.save_csv_results(stats)
        
        # åˆ›å»ºå¯è§†åŒ–
        benchmark.create_visualizations(stats, csv_filename)
        
        print("\næµ‹è¯•å®Œæˆ!")
    else:
        print("æµ‹è¯•å¤±è´¥")
        sys.exit(1)

if __name__ == "__main__":
    main()