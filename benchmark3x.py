#!/usr/bin/env python3
"""
Enhanced Deep Learning Benchmark Tool - å¢å¼ºç‰ˆæ·±åº¦å­¦ä¹ åŸºå‡†æµ‹è¯•å·¥å…·
æ”¯æŒå¤šç§æ¨¡å‹ç±»å‹å’Œæ•°æ®é›†çš„äº¤äº’å¼åŸºå‡†æµ‹è¯•

ä¸»è¦åŠŸèƒ½ï¼š
1. äº¤äº’å¼è®¾å¤‡é€‰æ‹© (CPU/GPU) - æ”¯æŒè¿”å›ä¸Šä¸€çº§
2. æ¨¡å‹ç±»å‹é€‰æ‹© (Detection/Classification) - æ”¯æŒè¿”å›ä¸Šä¸€çº§
3. æ•°æ®é›†é€‰æ‹© (MNIST/CIFAR-10/COCO/ImageNet) - æ”¯æŒè¿”å›ä¸Šä¸€çº§
4. è‡ªå®šä¹‰æ ·æœ¬æ•°é‡é€‰æ‹© - ä»100åˆ°å…¨éƒ¨æ•°æ®é›†
5. è¯¦ç»†æ€§èƒ½ç»Ÿè®¡å’ŒCSVè¾“å‡º - æŒ‰å¸§/å›¾åƒè¾“å‡ºè¯¦ç»†æ—¶é—´ä¿¡æ¯
6. ç»“æœå¯è§†åŒ–

ä¿®å¤å†…å®¹ï¼š
- ä¿®å¤MNISTæ•°æ®é›†çš„1é€šé“åˆ°3é€šé“è½¬æ¢é—®é¢˜
- æ·»åŠ äº†é€‚å½“çš„å›¾åƒå°ºå¯¸è°ƒæ•´
- æ”¹è¿›äº†æ•°æ®é¢„å¤„ç†æµç¨‹
- æ·»åŠ äº†å®Œå…¨å¯è‡ªå®šä¹‰çš„æ ·æœ¬æ•°é‡åŠŸèƒ½

Author: Enhanced for professional AI benchmarking
Platform: Ubuntu 22.04 + NVIDIA GPU support
"""

import argparse
import json
import csv
import os
import sys
import time
import socket
import threading
from pathlib import Path
from collections import deque
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
import seaborn as sns

# Deep Learning Libraries
import torch
import torch.nn as nn
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import timm

# System Monitoring
import psutil
try:
    import pynvml
    NVML_AVAILABLE = True
except ImportError:
    NVML_AVAILABLE = False
    print("Warning: pynvml not available. Install with: pip install nvidia-ml-py3")

# Object Detection (if available)
try:
    from ultralytics import YOLO
    ULTRALYTICS_AVAILABLE = True
except ImportError:
    ULTRALYTICS_AVAILABLE = False
    print("Warning: ultralytics not available. Install with: pip install ultralytics")


class GrayscaleToRGB(object):
    """å°†ç°åº¦å›¾åƒè½¬æ¢ä¸ºRGBå›¾åƒçš„å˜æ¢"""
    def __call__(self, img):
        if img.shape[0] == 1:  # å¦‚æœæ˜¯å•é€šé“
            return img.repeat(3, 1, 1)  # å¤åˆ¶åˆ°3ä¸ªé€šé“
        return img


class InteractiveBenchmark:
    def __init__(self):
        self.device = None
        self.model_type = None
        self.model = None
        self.dataset_name = None
        self.dataloader = None
        self.results = []
        self.detailed_results = []  # æ–°å¢ï¼šå­˜å‚¨è¯¦ç»†çš„æ¯å¸§ç»“æœ
        
        # Monitoring
        self.monitoring = True
        self.cpu_usage = deque(maxlen=1000)
        self.memory_usage = deque(maxlen=1000)
        self.gpu_memory = deque(maxlen=1000)
        self.gpu_utilization = deque(maxlen=1000)
        
        self.total_samples = 0
        self.start_time = None
        
        # å¯¼èˆªçŠ¶æ€ç®¡ç†
        self.setup_state = {
            'device': False,
            'model_type': False,
            'model': False,
            'dataset': False,
            'samples': False
        }
        
        # æµ‹è¯•æ ·æœ¬æ•°é‡è®¾ç½®
        self.test_samples = 100  # é»˜è®¤å€¼
        
        # Available models
        self.detection_models = {
            '1': {'name': 'YOLOv8n', 'model': 'yolov8n.pt', 'type': 'yolo'},
            '2': {'name': 'YOLOv8s', 'model': 'yolov8s.pt', 'type': 'yolo'},
            '3': {'name': 'YOLOv8m', 'model': 'yolov8m.pt', 'type': 'yolo'},
        }
        
        self.classification_models = {
            '1': {'name': 'ResNet18', 'model': 'resnet18', 'type': 'timm'},
            '2': {'name': 'ResNet50', 'model': 'resnet50', 'type': 'timm'},
            '3': {'name': 'EfficientNet-B0', 'model': 'efficientnet_b0', 'type': 'timm'},
            '4': {'name': 'EfficientNet-B3', 'model': 'efficientnet_b3', 'type': 'timm'},
            '5': {'name': 'Vision Transformer', 'model': 'vit_base_patch16_224', 'type': 'timm'},
            '6': {'name': 'MobileNet-V3', 'model': 'mobilenetv3_large_100', 'type': 'timm'},
        }

    def interactive_setup(self):
        """äº¤äº’å¼è®¾ç½® - æ”¯æŒè¿”å›ä¸Šä¸€çº§"""
        print("="*60)
        print("æ·±åº¦å­¦ä¹ æ¨¡å‹åŸºå‡†æµ‹è¯•å·¥å…·")
        print("="*60)
        print("æç¤ºï¼šåœ¨ä»»ä½•é€‰æ‹©é˜¶æ®µè¾“å…¥ 'b' æˆ– 'back' å¯è¿”å›ä¸Šä¸€æ­¥")
        print("="*60)
        
        # è®¾ç½®æµç¨‹çŠ¶æ€æœº
        current_step = 'device'
        
        while current_step != 'confirm':
            if current_step == 'device':
                result = self.select_device()
                if result == 'back':
                    print("å·²åœ¨ç¬¬ä¸€æ­¥ï¼Œæ— æ³•è¿”å›")
                    continue
                elif result == 'success':
                    self.setup_state['device'] = True
                    current_step = 'model_type'
                    
            elif current_step == 'model_type':
                result = self.select_model_type()
                if result == 'back':
                    current_step = 'device'
                    self.setup_state['model_type'] = False
                    continue
                elif result == 'success':
                    self.setup_state['model_type'] = True
                    current_step = 'model'
                    
            elif current_step == 'model':
                result = self.select_model()
                if result == 'back':
                    current_step = 'model_type'
                    self.setup_state['model'] = False
                    continue
                elif result == 'success':
                    self.setup_state['model'] = True
                    current_step = 'dataset'
                    
            elif current_step == 'dataset':
                result = self.select_dataset()
                if result == 'back':
                    current_step = 'model'
                    self.setup_state['dataset'] = False
                    continue
                elif result == 'success':
                    self.setup_state['dataset'] = True
                    current_step = 'samples'
                    
            elif current_step == 'samples':
                result = self.select_sample_count()
                if result == 'back':
                    current_step = 'dataset'
                    self.setup_state['samples'] = False
                    continue
                elif result == 'success':
                    self.setup_state['samples'] = True
                    current_step = 'confirm'
        
        # é…ç½®æ€»ç»“å’Œç¡®è®¤
        print("\n" + "="*60)
        print("é…ç½®æ€»ç»“ï¼š")
        print(f"è®¾å¤‡: {self.device}")
        print(f"æ¨¡å‹ç±»å‹: {self.model_type}")
        print(f"æ¨¡å‹: {self.model_info['name'] if hasattr(self, 'model_info') else 'Unknown'}")
        print(f"æ•°æ®é›†: {self.dataset_name}")
        print(f"æµ‹è¯•æ ·æœ¬æ•°: {self.test_samples if self.test_samples != -1 else 'å…¨éƒ¨'}")
        print("="*60)
        
        while True:
            confirm = input("\nç¡®è®¤å¼€å§‹æµ‹è¯•? (y/n/b-è¿”å›è®¾ç½®): ").lower().strip()
            if confirm == 'y':
                break
            elif confirm == 'n':
                print("æµ‹è¯•å·²å–æ¶ˆ")
                sys.exit(0)
            elif confirm in ['b', 'back']:
                # é‡æ–°å¼€å§‹è®¾ç½®æµç¨‹
                self.interactive_setup()
                return

    def select_device(self):
        """é€‰æ‹©è®¡ç®—è®¾å¤‡"""
        print("\n1. é€‰æ‹©è®¡ç®—è®¾å¤‡:")
        print("1) CPU")
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                print(f"2) CUDA:{i} - {torch.cuda.get_device_name(i)}")
        else:
            print("   (CUDAä¸å¯ç”¨)")
        
        while True:
            choice = input("è¯·é€‰æ‹©è®¾å¤‡ (è¾“å…¥æ•°å­—, 'b'è¿”å›): ").strip().lower()
            if choice in ['b', 'back']:
                return 'back'
            elif choice == '1':
                self.device = 'cpu'
                print(f"å·²é€‰æ‹©è®¾å¤‡: {self.device}")
                return 'success'
            elif choice == '2' and torch.cuda.is_available():
                self.device = 'cuda:0'
                print(f"å·²é€‰æ‹©è®¾å¤‡: {self.device}")
                return 'success'
            else:
                print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")

    def select_model_type(self):
        """é€‰æ‹©æ¨¡å‹ç±»å‹"""
        print("\n2. é€‰æ‹©æ¨¡å‹ç±»å‹:")
        print("1) å›¾åƒåˆ†ç±» (Classification)")
        if ULTRALYTICS_AVAILABLE:
            print("2) ç›®æ ‡æ£€æµ‹ (Object Detection)")
        else:
            print("2) ç›®æ ‡æ£€æµ‹ (éœ€è¦å®‰è£… ultralytics)")
        
        while True:
            choice = input("è¯·é€‰æ‹©æ¨¡å‹ç±»å‹ (è¾“å…¥æ•°å­—, 'b'è¿”å›): ").strip().lower()
            if choice in ['b', 'back']:
                return 'back'
            elif choice == '1':
                self.model_type = 'classification'
                print(f"å·²é€‰æ‹©æ¨¡å‹ç±»å‹: {self.model_type}")
                return 'success'
            elif choice == '2' and ULTRALYTICS_AVAILABLE:
                self.model_type = 'detection'
                print(f"å·²é€‰æ‹©æ¨¡å‹ç±»å‹: {self.model_type}")
                return 'success'
            elif choice == '2':
                print("ç›®æ ‡æ£€æµ‹éœ€è¦å®‰è£… ultralytics: pip install ultralytics")
            else:
                print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")

    def select_model(self):
        """é€‰æ‹©å…·ä½“æ¨¡å‹"""
        print(f"\n3. é€‰æ‹©{self.model_type}æ¨¡å‹:")
        
        if self.model_type == 'classification':
            for key, value in self.classification_models.items():
                print(f"{key}) {value['name']}")
            
            while True:
                choice = input("è¯·é€‰æ‹©æ¨¡å‹ (è¾“å…¥æ•°å­—, 'b'è¿”å›): ").strip().lower()
                if choice in ['b', 'back']:
                    return 'back'
                elif choice in self.classification_models:
                    selected = self.classification_models[choice]
                    self.model_info = selected
                    print(f"å·²é€‰æ‹©æ¨¡å‹: {selected['name']}")
                    return 'success'
                else:
                    print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")
                    
        elif self.model_type == 'detection':
            for key, value in self.detection_models.items():
                print(f"{key}) {value['name']}")
            
            while True:
                choice = input("è¯·é€‰æ‹©æ¨¡å‹ (è¾“å…¥æ•°å­—, 'b'è¿”å›): ").strip().lower()
                if choice in ['b', 'back']:
                    return 'back'
                elif choice in self.detection_models:
                    selected = self.detection_models[choice]
                    self.model_info = selected
                    print(f"å·²é€‰æ‹©æ¨¡å‹: {selected['name']}")
                    return 'success'
                else:
                    print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")

    def select_dataset(self):
        """é€‰æ‹©æ•°æ®é›†"""
        print("\n4. é€‰æ‹©æ•°æ®é›†:")
        
        if self.model_type == 'classification':
            print("1) MNIST (æ‰‹å†™æ•°å­—, 28x28 -> 224x224)")
            print("2) CIFAR-10 (å°ç‰©ä½“åˆ†ç±», 32x32 -> 224x224)")
            print("3) ImageNet éªŒè¯é›†æ ·æœ¬ (224x224)")
            print("æ³¨æ„ï¼šå®é™…æ•°æ®é›†å¤§å°å–å†³äºä¸‹ä¸€æ­¥çš„æ ·æœ¬æ•°é‡è®¾ç½®")
            
            datasets = {
                '1': {'name': 'MNIST', 'func': self.load_mnist},
                '2': {'name': 'CIFAR-10', 'func': self.load_cifar10},
                '3': {'name': 'ImageNet-Sample', 'func': self.load_imagenet_sample}
            }
            
        elif self.model_type == 'detection':
            print("1) COCO éªŒè¯é›†æ ·æœ¬ (éœ€è¦ä¸‹è½½)")
            print("2) é¢„è®¾æµ‹è¯•å›¾åƒ")
            print("æ³¨æ„ï¼šå®é™…å›¾åƒæ•°é‡å–å†³äºä¸‹ä¸€æ­¥çš„æ ·æœ¬æ•°é‡è®¾ç½®")
            
            datasets = {
                '1': {'name': 'COCO-Sample', 'func': self.load_coco_sample},
                '2': {'name': 'Test-Images', 'func': self.load_test_images}
            }
        
        while True:
            choice = input("è¯·é€‰æ‹©æ•°æ®é›† (è¾“å…¥æ•°å­—, 'b'è¿”å›): ").strip().lower()
            if choice in ['b', 'back']:
                return 'back'
            elif choice in datasets:
                selected = datasets[choice]
                self.dataset_name = selected['name']
                self.load_dataset_func = selected['func']
                print(f"å·²é€‰æ‹©æ•°æ®é›†: {selected['name']}")
                return 'success'
            else:
                print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")

    def select_sample_count(self):
        """é€‰æ‹©æµ‹è¯•æ ·æœ¬æ•°é‡"""
        print("\n5. é€‰æ‹©æµ‹è¯•æ ·æœ¬æ•°é‡:")
        print("1) å¿«é€Ÿæµ‹è¯• (100 æ ·æœ¬)")
        print("2) ä¸­ç­‰æµ‹è¯• (500 æ ·æœ¬)")
        print("3) æ ‡å‡†æµ‹è¯• (1000 æ ·æœ¬)")
        print("4) å¤§è§„æ¨¡æµ‹è¯• (5000 æ ·æœ¬)")
        print("5) å…¨éƒ¨æ ·æœ¬ (ä½¿ç”¨å®Œæ•´æ•°æ®é›†)")
        print("6) è‡ªå®šä¹‰æ•°é‡")
        
        while True:
            choice = input("è¯·é€‰æ‹©æµ‹è¯•è§„æ¨¡ (è¾“å…¥æ•°å­—, 'b'è¿”å›): ").strip().lower()
            if choice in ['b', 'back']:
                return 'back'
            elif choice == '1':
                self.test_samples = 100
                print(f"å·²é€‰æ‹©æµ‹è¯•æ ·æœ¬æ•°: {self.test_samples}")
                return 'success'
            elif choice == '2':
                self.test_samples = 500
                print(f"å·²é€‰æ‹©æµ‹è¯•æ ·æœ¬æ•°: {self.test_samples}")
                return 'success'
            elif choice == '3':
                self.test_samples = 1000
                print(f"å·²é€‰æ‹©æµ‹è¯•æ ·æœ¬æ•°: {self.test_samples}")
                return 'success'
            elif choice == '4':
                self.test_samples = 5000
                print(f"å·²é€‰æ‹©æµ‹è¯•æ ·æœ¬æ•°: {self.test_samples}")
                return 'success'
            elif choice == '5':
                self.test_samples = -1  # -1 è¡¨ç¤ºå…¨éƒ¨æ ·æœ¬
                print("å·²é€‰æ‹©æµ‹è¯•æ ·æœ¬æ•°: å…¨éƒ¨")
                return 'success'
            elif choice == '6':
                while True:
                    try:
                        custom_count = input("è¯·è¾“å…¥è‡ªå®šä¹‰æ ·æœ¬æ•°é‡ (è¾“å…¥ 'b' è¿”å›): ").strip()
                        if custom_count.lower() in ['b', 'back']:
                            break
                        custom_count = int(custom_count)
                        if custom_count > 0:
                            self.test_samples = custom_count
                            print(f"å·²é€‰æ‹©æµ‹è¯•æ ·æœ¬æ•°: {self.test_samples}")
                            return 'success'
                        else:
                            print("æ ·æœ¬æ•°é‡å¿…é¡»å¤§äº0")
                    except ValueError:
                        print("è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
            else:
                print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")

    def load_model(self):
        """åŠ è½½é€‰å®šçš„æ¨¡å‹"""
        print(f"\næ­£åœ¨åŠ è½½æ¨¡å‹: {self.model_info['name']}...")
        
        try:
            if self.model_type == 'classification':
                if self.model_info['type'] == 'timm':
                    self.model = timm.create_model(
                        self.model_info['model'], 
                        pretrained=True,
                        num_classes=1000
                    )
                    self.model.eval()
                    self.model = self.model.to(self.device)
                    
            elif self.model_type == 'detection':
                if self.model_info['type'] == 'yolo':
                    self.model = YOLO(self.model_info['model'])
                    
            print("æ¨¡å‹åŠ è½½æˆåŠŸ!")
            
        except Exception as e:
            print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            sys.exit(1)

    def load_mnist(self):
        """åŠ è½½MNISTæ•°æ®é›† - ä¿®å¤é€šé“æ•°é—®é¢˜"""
        print("æ­£åœ¨åŠ è½½MNISTæ•°æ®é›†...")
        print("æ³¨æ„ï¼šå°†ç°åº¦å›¾åƒ(1é€šé“)è½¬æ¢ä¸ºRGBå›¾åƒ(3é€šé“)å¹¶è°ƒæ•´å¤§å°åˆ°224x224")
        
        # å¯¹äºMNISTï¼Œéœ€è¦ç‰¹æ®Šçš„é¢„å¤„ç†ï¼š1é€šé“->3é€šé“ï¼Œ28x28->224x224
        transform = transforms.Compose([
            transforms.ToTensor(),  # è½¬æ¢ä¸ºtensor (0-1èŒƒå›´)
            transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.shape[0] == 1 else x),  # 1é€šé“->3é€šé“
            transforms.Resize((224, 224)),  # è°ƒæ•´å¤§å°åˆ°224x224
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNetæ ‡å‡†åŒ–
        ])
        
        dataset = torchvision.datasets.MNIST(
            root='./data', train=False, download=True, transform=transform
        )
        self.dataloader = DataLoader(dataset, batch_size=1, shuffle=False)
        print(f"MNISTæ•°æ®é›†åŠ è½½å®Œæˆï¼Œå…±{len(dataset)}ä¸ªæ ·æœ¬")
        print(f"å°†æ ¹æ®ç”¨æˆ·è®¾ç½®æµ‹è¯• {self.test_samples if self.test_samples != -1 else len(dataset)} ä¸ªæ ·æœ¬")
        print("æ•°æ®é¢„å¤„ç†ï¼šç°åº¦->RGB, 28x28->224x224")

    def load_cifar10(self):
        """åŠ è½½CIFAR-10æ•°æ®é›† - ä¿®å¤å°ºå¯¸é—®é¢˜"""
        print("æ­£åœ¨åŠ è½½CIFAR-10æ•°æ®é›†...")
        print("æ³¨æ„ï¼šå°†å›¾åƒä»32x32è°ƒæ•´åˆ°224x224")
        
        # å¯¹äºCIFAR-10ï¼Œéœ€è¦è°ƒæ•´å°ºå¯¸ï¼š32x32->224x224
        transform = transforms.Compose([
            transforms.ToTensor(),  # è½¬æ¢ä¸ºtensor
            transforms.Resize((224, 224)),  # è°ƒæ•´å¤§å°åˆ°224x224
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNetæ ‡å‡†åŒ–
        ])
        
        dataset = torchvision.datasets.CIFAR10(
            root='./data', train=False, download=True, transform=transform
        )
        self.dataloader = DataLoader(dataset, batch_size=1, shuffle=False)
        print(f"CIFAR-10æ•°æ®é›†åŠ è½½å®Œæˆï¼Œå…±{len(dataset)}ä¸ªæ ·æœ¬")
        print(f"å°†æ ¹æ®ç”¨æˆ·è®¾ç½®æµ‹è¯• {self.test_samples if self.test_samples != -1 else len(dataset)} ä¸ªæ ·æœ¬")
        print("æ•°æ®é¢„å¤„ç†ï¼š32x32->224x224")

    def load_imagenet_sample(self):
        """åŠ è½½ImageNetæ ·æœ¬æ•°æ®é›†"""
        print("ImageNetæ ·æœ¬æ•°æ®é›†è®¾ç½®...")
        print("è¯·ä¸‹è½½ImageNetéªŒè¯é›†åˆ° './data/imagenet/val' ç›®å½•")
        print("ä¸‹è½½åœ°å€: https://www.image-net.org/download.php")
        print("æˆ–ä½¿ç”¨å°‘é‡æ ·æœ¬å›¾åƒè¿›è¡Œæµ‹è¯•")
        
        # åˆ›å»ºç®€å•çš„æµ‹è¯•æ•°æ®
        self.create_synthetic_dataset(224, 1000)

    def load_coco_sample(self):
        """åŠ è½½COCOæ ·æœ¬æ•°æ®é›†"""
        print("COCOæ ·æœ¬æ•°æ®é›†è®¾ç½®...")
        print("è¯·ä¸‹è½½COCO 2017éªŒè¯é›†åˆ° './data/coco' ç›®å½•")
        print("ä¸‹è½½åœ°å€: http://cocodataset.org/#download")
        
        # åˆ›å»ºç®€å•çš„æµ‹è¯•æ•°æ®
        self.create_synthetic_detection_data()

    def load_test_images(self):
        """åŠ è½½é¢„è®¾æµ‹è¯•å›¾åƒ"""
        print("ä½¿ç”¨é¢„è®¾æµ‹è¯•å›¾åƒ...")
        self.create_synthetic_detection_data()

    def create_synthetic_dataset(self, img_size, num_classes):
        """åˆ›å»ºåˆæˆæ•°æ®é›†ç”¨äºæµ‹è¯•"""
        # æ ¹æ®ç”¨æˆ·é€‰æ‹©ç¡®å®šæ•°æ®é›†å¤§å°
        if self.test_samples == -1:
            dataset_size = 10000  # å…¨éƒ¨æ ·æœ¬æ—¶ä½¿ç”¨10000ä½œä¸ºé»˜è®¤å¤§å°
            print(f"åˆ›å»ºåˆæˆæ•°æ®é›† ({img_size}x{img_size}, {num_classes}ç±», {dataset_size}ä¸ªæ ·æœ¬)...")
        else:
            dataset_size = max(self.test_samples, 100)  # è‡³å°‘100ä¸ªæ ·æœ¬
            print(f"åˆ›å»ºåˆæˆæ•°æ®é›† ({img_size}x{img_size}, {num_classes}ç±», {dataset_size}ä¸ªæ ·æœ¬)...")
        
        class SyntheticDataset(torch.utils.data.Dataset):
            def __init__(self, size, img_size=224, num_classes=1000):
                self.size = size
                self.img_size = img_size
                self.num_classes = num_classes
                
            def __len__(self):
                return self.size
                
            def __getitem__(self, idx):
                # ç”Ÿæˆéšæœºå›¾åƒ (3é€šé“)
                img = torch.randn(3, self.img_size, self.img_size)
                # æ·»åŠ ImageNetæ ‡å‡†åŒ–
                normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
                img = normalize(img)
                label = torch.randint(0, self.num_classes, (1,)).item()
                return img, label
        
        dataset = SyntheticDataset(dataset_size, img_size, num_classes)
        self.dataloader = DataLoader(dataset, batch_size=1, shuffle=False)
        print("åˆæˆæ•°æ®é›†åˆ›å»ºå®Œæˆ")

    def create_synthetic_detection_data(self):
        """åˆ›å»ºåˆæˆæ£€æµ‹æ•°æ®"""
        # æ ¹æ®ç”¨æˆ·é€‰æ‹©ç¡®å®šæµ‹è¯•å›¾åƒæ•°é‡
        if self.test_samples == -1:
            num_images = 1000  # å…¨éƒ¨æ ·æœ¬æ—¶ä½¿ç”¨1000ä½œä¸ºé»˜è®¤å¤§å°
            print(f"åˆ›å»ºåˆæˆæ£€æµ‹æ•°æ®... ({num_images}å¼ æµ‹è¯•å›¾åƒ)")
        else:
            num_images = max(self.test_samples, 10)  # è‡³å°‘10å¼ å›¾åƒ
            print(f"åˆ›å»ºåˆæˆæ£€æµ‹æ•°æ®... ({num_images}å¼ æµ‹è¯•å›¾åƒ)")
        
        # ç”Ÿæˆæµ‹è¯•å›¾åƒè·¯å¾„
        self.test_images = []
        for i in range(num_images):
            # åˆ›å»ºéšæœºå›¾åƒå¹¶ä¿å­˜
            img = np.random.randint(0, 255, (640, 640, 3), dtype=np.uint8)
            img_path = f"./temp_test_img_{i}.jpg"
            
            # è¿™é‡Œåº”è¯¥ä¿å­˜å›¾åƒï¼Œä½†ä¸ºäº†ç®€åŒ–æˆ‘ä»¬åªä¿å­˜è·¯å¾„
            self.test_images.append(img_path)
        
        print(f"åˆ›å»º{len(self.test_images)}ä¸ªæµ‹è¯•å›¾åƒ")

    def monitor_resources(self):
        """ç›‘æ§ç³»ç»Ÿèµ„æº"""
        if torch.cuda.is_available() and NVML_AVAILABLE:
            try:
                pynvml.nvmlInit()
                handle = pynvml.nvmlDeviceGetHandleByIndex(0)
            except:
                handle = None
        else:
            handle = None
        
        process = psutil.Process()
        
        while self.monitoring:
            try:
                # CPUå’Œå†…å­˜
                self.cpu_usage.append(process.cpu_percent(interval=0.1))
                self.memory_usage.append(psutil.virtual_memory().percent)
                
                # GPUç›‘æ§
                if torch.cuda.is_available():
                    # GPUå†…å­˜
                    gpu_mem = torch.cuda.memory_allocated(0)
                    gpu_total = torch.cuda.get_device_properties(0).total_memory
                    self.gpu_memory.append((gpu_mem / gpu_total) * 100)
                    
                    # GPUåˆ©ç”¨ç‡
                    if handle:
                        try:
                            util = pynvml.nvmlDeviceGetUtilizationRates(handle)
                            self.gpu_utilization.append(util.gpu)
                        except:
                            self.gpu_utilization.append(0)
                
                time.sleep(0.1)
                
            except Exception as e:
                break

    def run_classification_benchmark(self):
        """è¿è¡Œåˆ†ç±»æ¨¡å‹åŸºå‡†æµ‹è¯•"""
        print("\nå¼€å§‹åˆ†ç±»æ¨¡å‹åŸºå‡†æµ‹è¯•...")
        print(f"æ­£åœ¨ä½¿ç”¨æ¨¡å‹: {self.model_info['name']}")
        print(f"è®¡åˆ’æµ‹è¯•æ ·æœ¬æ•°: {self.test_samples if self.test_samples != -1 else 'å…¨éƒ¨'}")
        print(f"è¾“å…¥æ•°æ®æ ¼å¼éªŒè¯ä¸­...")
        
        batch_times = []
        preprocessing_times = []
        inference_times = []
        
        self.model.eval()
        with torch.no_grad():
            for batch_idx, (data, target) in enumerate(self.dataloader):
                batch_start = time.time()
                
                # éªŒè¯è¾“å…¥æ•°æ®å½¢çŠ¶
                if batch_idx == 0:
                    print(f"è¾“å…¥æ•°æ®å½¢çŠ¶: {data.shape}")
                    print(f"æ•°æ®ç±»å‹: {data.dtype}")
                    print(f"æ•°æ®èŒƒå›´: [{data.min().item():.3f}, {data.max().item():.3f}]")
                
                # é¢„å¤„ç†æ—¶é—´
                prep_start = time.time()
                data = data.to(self.device)
                prep_time = (time.time() - prep_start) * 1000  # ms
                
                # æ¨ç†æ—¶é—´
                inf_start = time.time()
                try:
                    output = self.model(data)
                    inf_time = (time.time() - inf_start) * 1000  # ms
                except Exception as e:
                    print(f"æ¨ç†è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
                    print(f"è¾“å…¥å½¢çŠ¶: {data.shape}")
                    print(f"è®¾å¤‡: {data.device}")
                    raise e
                
                batch_time = (time.time() - batch_start) * 1000  # ms
                
                # ç¡®ä¿æ—¶é—´å€¼åˆç†ï¼Œé¿å…å¼‚å¸¸æ•°æ®
                prep_time = max(prep_time, 0.001)  # æœ€å°0.001ms
                inf_time = max(inf_time, 0.001)
                batch_time = max(batch_time, 0.001)
                
                # è®°å½•è¯¦ç»†ç»“æœï¼ˆæ¯ä¸ªæ ·æœ¬ï¼‰
                batch_size = len(data)
                if batch_size > 0:  # é˜²æ­¢é™¤é›¶é”™è¯¯
                    for i in range(batch_size):
                        sample_prep_time = prep_time / batch_size
                        sample_inf_time = inf_time / batch_size  
                        sample_total_time = batch_time / batch_size
                        
                        # ç¡®ä¿æ¯ä¸ªæ ·æœ¬æ—¶é—´éƒ½æ˜¯åˆç†çš„
                        sample_prep_time = max(sample_prep_time, 0.001)
                        sample_inf_time = max(sample_inf_time, 0.001)
                        sample_total_time = max(sample_total_time, 0.001)
                        
                        self.detailed_results.append({
                            'sample_id': self.total_samples + i,
                            'preprocessing_time': sample_prep_time,
                            'inference_time': sample_inf_time,
                            'postprocessing_time': 0.0,  # åˆ†ç±»ä»»åŠ¡æ— åå¤„ç†
                            'total_time': sample_total_time
                        })
                
                # è®°å½•æ±‡æ€»æ—¶é—´
                preprocessing_times.append(prep_time)
                inference_times.append(inf_time)
                batch_times.append(batch_time)
                
                self.total_samples += len(data)
                
                # è¿›åº¦æ˜¾ç¤º
                if batch_idx % 10 == 0:
                    fps = 1000.0 / (batch_time / len(data)) if batch_time > 0 else 0
                    if self.test_samples == -1:
                        print(f"Processed {self.total_samples} samples... å½“å‰FPS: {fps:.1f}")
                    else:
                        progress = (self.total_samples / self.test_samples) * 100
                        print(f"Processed {self.total_samples}/{self.test_samples} samples ({progress:.1f}%)... å½“å‰FPS: {fps:.1f}")
                
                # æ ¹æ®ç”¨æˆ·è®¾ç½®é™åˆ¶æµ‹è¯•æ ·æœ¬æ•°
                if self.test_samples != -1 and self.total_samples >= self.test_samples:
                    print(f"è¾¾åˆ°ç›®æ ‡æ ·æœ¬æ•° {self.test_samples}ï¼Œæµ‹è¯•å®Œæˆ")
                    break
        
        return {
            'preprocessing_times': preprocessing_times,
            'inference_times': inference_times,
            'batch_times': batch_times
        }

    def run_detection_benchmark(self):
        """è¿è¡Œæ£€æµ‹æ¨¡å‹åŸºå‡†æµ‹è¯•"""
        print("\nå¼€å§‹æ£€æµ‹æ¨¡å‹åŸºå‡†æµ‹è¯•...")
        print(f"è®¡åˆ’æµ‹è¯•å›¾åƒæ•°: {self.test_samples if self.test_samples != -1 else len(self.test_images)}")
        
        preprocessing_times = []
        inference_times = []
        postprocessing_times = []
        
        # ç¡®å®šå®é™…è¦æµ‹è¯•çš„å›¾åƒæ•°é‡
        if self.test_samples == -1:
            num_test_images = len(self.test_images)
        else:
            num_test_images = min(self.test_samples, len(self.test_images))
        
        print(f"å®é™…æµ‹è¯•å›¾åƒæ•°: {num_test_images}")
        
        # ä½¿ç”¨åˆæˆå›¾åƒè¿›è¡Œæµ‹è¯•
        for i in range(num_test_images):
            # åˆ›å»ºéšæœºå›¾åƒè¿›è¡Œæµ‹è¯•
            img = np.random.randint(0, 255, (640, 640, 3), dtype=np.uint8)
            
            # è®°å½•æ€»æ—¶é—´
            total_start = time.time()
            results = self.model(img, device=self.device, verbose=False)
            total_elapsed = (time.time() - total_start) * 1000  # ms
            
            # è·å–æ—¶é—´ä¿¡æ¯
            prep_time = 0.0
            inf_time = total_elapsed  # é»˜è®¤å€¼
            post_time = 0.0
            
            if hasattr(results[0], 'speed'):
                speed = results[0].speed
                prep_time = speed.get('preprocess', 0)
                inf_time = speed.get('inference', 0)
                post_time = speed.get('postprocess', 0)
            
            # ç¡®ä¿æ—¶é—´å€¼åˆç†
            prep_time = max(prep_time, 0.001)
            inf_time = max(inf_time, 0.001) 
            post_time = max(post_time, 0.001)
            total_time = prep_time + inf_time + post_time
            
            # å¦‚æœæ€»æ—¶é—´å¼‚å¸¸ï¼Œä½¿ç”¨å®é™…æµ‹é‡æ—¶é—´
            if total_time < total_elapsed * 0.5:  # å¦‚æœæ€»æ—¶é—´æ˜æ˜¾å°äºå®é™…æ—¶é—´
                total_time = max(total_elapsed, 0.001)
                inf_time = total_time - prep_time - post_time
                inf_time = max(inf_time, 0.001)
            
            # è®°å½•è¯¦ç»†ç»“æœ
            self.detailed_results.append({
                'sample_id': i,
                'preprocessing_time': prep_time,
                'inference_time': inf_time,
                'postprocessing_time': post_time,
                'total_time': total_time
            })
            
            preprocessing_times.append(prep_time)
            inference_times.append(inf_time)
            postprocessing_times.append(post_time)
            
            self.total_samples += 1
            
            # è¿›åº¦æ˜¾ç¤º
            if i % 10 == 0 or i == num_test_images - 1:
                fps = 1000.0 / total_time if total_time > 0 else 0
                progress = ((i + 1) / num_test_images) * 100
                print(f"Processed {i + 1}/{num_test_images} images ({progress:.1f}%)... å½“å‰FPS: {fps:.1f}")
        
        return {
            'preprocessing_times': preprocessing_times,
            'inference_times': inference_times,
            'postprocessing_times': postprocessing_times
        }

    def run_benchmark(self):
        """è¿è¡ŒåŸºå‡†æµ‹è¯•"""
        print("\nå¼€å§‹åŸºå‡†æµ‹è¯•...")
        
        # å¯åŠ¨èµ„æºç›‘æ§
        monitor_thread = threading.Thread(target=self.monitor_resources)
        monitor_thread.daemon = True
        monitor_thread.start()
        
        self.start_time = time.time()
        
        try:
            if self.model_type == 'classification':
                timing_results = self.run_classification_benchmark()
            elif self.model_type == 'detection':
                timing_results = self.run_detection_benchmark()
            
        except KeyboardInterrupt:
            print("\næµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
            return None
        except Exception as e:
            print(f"æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return None
        finally:
            self.monitoring = False
            time.sleep(0.5)  # ç­‰å¾…ç›‘æ§çº¿ç¨‹ç»“æŸ
        
        end_time = time.time()
        total_time = end_time - self.start_time
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        stats = self.calculate_statistics(timing_results, total_time)
        
        return stats

    def calculate_statistics(self, timing_results, total_time):
        """è®¡ç®—ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'system_info': {
                'hostname': socket.gethostname(),
                'device': self.device,
                'model_type': self.model_type,
                'model_name': self.model_info['name'],
                'dataset': self.dataset_name,
                'torch_version': torch.__version__,
                'cuda_available': torch.cuda.is_available(),
                'device_name': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'
            },
            'performance': {
                'total_samples': self.total_samples,
                'total_time': total_time,
                'throughput': self.total_samples / total_time if total_time > 0 else 0,
                'avg_time_per_sample': (total_time / self.total_samples * 1000) if self.total_samples > 0 else 0
            },
            'timing': {},
            'resources': {
                'cpu': {
                    'min': np.min(self.cpu_usage) if self.cpu_usage else 0,
                    'max': np.max(self.cpu_usage) if self.cpu_usage else 0,
                    'avg': np.mean(self.cpu_usage) if self.cpu_usage else 0,
                    'std': np.std(self.cpu_usage) if self.cpu_usage else 0
                },
                'memory': {
                    'min': np.min(self.memory_usage) if self.memory_usage else 0,
                    'max': np.max(self.memory_usage) if self.memory_usage else 0,
                    'avg': np.mean(self.memory_usage) if self.memory_usage else 0,
                    'std': np.std(self.memory_usage) if self.memory_usage else 0
                }
            }
        }
        
        # æ·»åŠ æ—¶é—´ç»Ÿè®¡
        for key, times in timing_results.items():
            if times:
                stats['timing'][key] = {
                    'min': np.min(times),
                    'max': np.max(times),
                    'avg': np.mean(times),
                    'std': np.std(times)
                }
        
        # GPUç»Ÿè®¡
        if torch.cuda.is_available() and self.gpu_memory:
            stats['resources']['gpu'] = {
                'memory': {
                    'min': np.min(self.gpu_memory),
                    'max': np.max(self.gpu_memory),
                    'avg': np.mean(self.gpu_memory),
                    'std': np.std(self.gpu_memory)
                },
                'utilization': {
                    'min': np.min(self.gpu_utilization) if self.gpu_utilization else 0,
                    'max': np.max(self.gpu_utilization) if self.gpu_utilization else 0,
                    'avg': np.mean(self.gpu_utilization) if self.gpu_utilization else 0,
                    'std': np.std(self.gpu_utilization) if self.gpu_utilization else 0
                }
            }
        
        return stats

    def print_concise_results(self, stats):
        """æ‰“å°ç®€æ´çš„ç»“æœ"""
        print("\n" + "="*70)
        print("BENCHMARK RESULTS SUMMARY")
        print("="*70)
        
        # åŸºæœ¬ä¿¡æ¯
        print(f"Model: {stats['system_info']['model_name']}")
        print(f"Dataset: {stats['system_info']['dataset']}")
        print(f"Device: {stats['system_info']['device']}")
        print(f"Device Name: {stats['system_info']['device_name']}")
        
        # æ€§èƒ½æŒ‡æ ‡
        print(f"\n{'='*20} PERFORMANCE METRICS {'='*20}")
        print(f"  Samples processed: {stats['performance']['total_samples']}")
        print(f"  Total time: {stats['performance']['total_time']:.2f}s")
        print(f"  Throughput: {stats['performance']['throughput']:.2f} samples/sec")
        print(f"  Avg time/sample: {stats['performance']['avg_time_per_sample']:.2f}ms")
        
        # æ—¶é—´åˆ†è§£
        if stats['timing']:
            print(f"\n{'='*20} TIMING BREAKDOWN (ms) {'='*20}")
            for stage, data in stats['timing'].items():
                stage_name = stage.replace('_', ' ').title()
                print(f"  {stage_name}:")
                print(f"    Min: {data['min']:.2f}ms")
                print(f"    Max: {data['max']:.2f}ms")
                print(f"    Avg: {data['avg']:.2f}ms Â± {data['std']:.2f}")
                print()
        
        # èµ„æºä½¿ç”¨
        print(f"{'='*20} RESOURCE UTILIZATION {'='*20}")
        print(f"  CPU Usage:")
        print(f"    Min: {stats['resources']['cpu']['min']:.1f}%")
        print(f"    Max: {stats['resources']['cpu']['max']:.1f}%")
        print(f"    Avg: {stats['resources']['cpu']['avg']:.1f}% Â± {stats['resources']['cpu']['std']:.1f}")
        print()
        
        print(f"  Memory Usage:")
        print(f"    Min: {stats['resources']['memory']['min']:.1f}%")
        print(f"    Max: {stats['resources']['memory']['max']:.1f}%")
        print(f"    Avg: {stats['resources']['memory']['avg']:.1f}% Â± {stats['resources']['memory']['std']:.1f}")
        print()
        
        if 'gpu' in stats['resources']:
            print(f"  GPU Memory:")
            print(f"    Min: {stats['resources']['gpu']['memory']['min']:.1f}%")
            print(f"    Max: {stats['resources']['gpu']['memory']['max']:.1f}%")
            print(f"    Avg: {stats['resources']['gpu']['memory']['avg']:.1f}% Â± {stats['resources']['gpu']['memory']['std']:.1f}")
            print()
            
            print(f"  GPU Utilization:")
            print(f"    Min: {stats['resources']['gpu']['utilization']['min']:.1f}%")
            print(f"    Max: {stats['resources']['gpu']['utilization']['max']:.1f}%")
            print(f"    Avg: {stats['resources']['gpu']['utilization']['avg']:.1f}% Â± {stats['resources']['gpu']['utilization']['std']:.1f}")
            print()
        
        # æ€§èƒ½è¯„çº§
        fps = stats['performance']['throughput']
        if self.model_type == 'classification':
            if fps > 100: rating = "Excellent ğŸŸ¢"
            elif fps > 50: rating = "Good ğŸŸ¡"
            elif fps > 10: rating = "Fair ğŸŸ "
            else: rating = "Slow ğŸ”´"
        else:  # detection
            if fps > 30: rating = "Excellent ğŸŸ¢"
            elif fps > 15: rating = "Good ğŸŸ¡"
            elif fps > 5: rating = "Fair ğŸŸ "
            else: rating = "Slow ğŸ”´"
        
        print(f"{'='*20} OVERALL RATING {'='*25}")
        print(f"  Performance Rating: {rating}")
        print("="*70)

    def save_detailed_csv_results(self, stats):
        """ä¿å­˜è¯¦ç»†çš„CSVç»“æœ - æŒ‰å¸§/å›¾åƒè¾“å‡º"""
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        hostname = socket.gethostname()
        
        # è¯¦ç»†ç»“æœæ–‡ä»¶
        detailed_filename = f"{hostname}_{self.model_type}_detailed_{timestamp}.csv"
        
        with open(detailed_filename, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            
            # å†™å…¥è¡¨å¤´
            if self.model_type == 'detection':
                writer.writerow(['Image_ID', 'Preprocessing_Time_ms', 'Inference_Time_ms', 'Postprocessing_Time_ms', 'Total_Time_ms'])
            else:
                writer.writerow(['Sample_ID', 'Preprocessing_Time_ms', 'Inference_Time_ms', 'Postprocessing_Time_ms', 'Total_Time_ms'])
            
            # å†™å…¥è¯¦ç»†æ•°æ®
            for result in self.detailed_results:
                writer.writerow([
                    result['sample_id'],
                    f"{result['preprocessing_time']:.4f}",
                    f"{result['inference_time']:.4f}",
                    f"{result['postprocessing_time']:.4f}",
                    f"{result['total_time']:.4f}"
                ])
        
        print(f"\nè¯¦ç»†ç»“æœå·²ä¿å­˜è‡³: {detailed_filename}")
        
        # æ±‡æ€»ç»Ÿè®¡æ–‡ä»¶
        summary_filename = f"{hostname}_{self.model_type}_summary_{timestamp}.csv"
        
        with open(summary_filename, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            
            # ç³»ç»Ÿä¿¡æ¯éƒ¨åˆ†
            writer.writerow(['=== SYSTEM INFORMATION ==='])
            writer.writerow(['Metric', 'Value', 'Unit'])
            writer.writerow(['Hostname', stats['system_info']['hostname'], ''])
            writer.writerow(['Model Type', stats['system_info']['model_type'], ''])
            writer.writerow(['Model Name', stats['system_info']['model_name'], ''])
            writer.writerow(['Dataset', stats['system_info']['dataset'], ''])
            writer.writerow(['Device', stats['system_info']['device'], ''])
            writer.writerow(['Device Name', stats['system_info']['device_name'], ''])
            writer.writerow(['PyTorch Version', stats['system_info']['torch_version'], ''])
            writer.writerow(['CUDA Available', stats['system_info']['cuda_available'], ''])
            writer.writerow([])  # ç©ºè¡Œåˆ†éš”
            
            # æ€§èƒ½æŒ‡æ ‡éƒ¨åˆ†
            writer.writerow(['=== PERFORMANCE METRICS ==='])
            writer.writerow(['Metric', 'Value', 'Unit'])
            writer.writerow(['Total Samples', stats['performance']['total_samples'], 'samples'])
            writer.writerow(['Total Time', f"{stats['performance']['total_time']:.4f}", 'seconds'])
            writer.writerow(['Throughput', f"{stats['performance']['throughput']:.4f}", 'samples/sec'])
            writer.writerow(['Avg Time per Sample', f"{stats['performance']['avg_time_per_sample']:.4f}", 'ms'])
            writer.writerow([])  # ç©ºè¡Œåˆ†éš”
            
            # æ—¶é—´åˆ†è§£éƒ¨åˆ†
            if stats['timing']:
                writer.writerow(['=== TIMING BREAKDOWN ==='])
                writer.writerow(['Stage', 'Min (ms)', 'Max (ms)', 'Avg (ms)', 'Std (ms)'])
                for stage, data in stats['timing'].items():
                    stage_name = stage.replace('_', ' ').title()
                    writer.writerow([
                        stage_name,
                        f"{data['min']:.4f}",
                        f"{data['max']:.4f}",
                        f"{data['avg']:.4f}",
                        f"{data['std']:.4f}"
                    ])
                writer.writerow([])  # ç©ºè¡Œåˆ†éš”
            
            # èµ„æºä½¿ç”¨éƒ¨åˆ†
            writer.writerow(['=== RESOURCE UTILIZATION ==='])
            writer.writerow(['Resource', 'Min (%)', 'Max (%)', 'Avg (%)'])
            writer.writerow(['CPU Usage', 
                           f"{stats['resources']['cpu']['min']:.2f}", 
                           f"{stats['resources']['cpu']['max']:.2f}", 
                           f"{stats['resources']['cpu']['avg']:.2f}"])
            writer.writerow(['Memory Usage', 
                           f"{stats['resources']['memory']['min']:.2f}", 
                           f"{stats['resources']['memory']['max']:.2f}", 
                           f"{stats['resources']['memory']['avg']:.2f}"])
            
            if 'gpu' in stats['resources']:
                writer.writerow(['GPU Memory', 
                               f"{stats['resources']['gpu']['memory']['min']:.2f}", 
                               f"{stats['resources']['gpu']['memory']['max']:.2f}", 
                               f"{stats['resources']['gpu']['memory']['avg']:.2f}"])
                writer.writerow(['GPU Utilization', 
                               f"{stats['resources']['gpu']['utilization']['min']:.2f}", 
                               f"{stats['resources']['gpu']['utilization']['max']:.2f}", 
                               f"{stats['resources']['gpu']['utilization']['avg']:.2f}"])
        
        print(f"æ±‡æ€»ç»“æœå·²ä¿å­˜è‡³: {summary_filename}")
        return detailed_filename, summary_filename

    def create_detailed_timing_plot(self, timestamp, hostname):
        """åˆ›å»ºè¯¦ç»†çš„æ¯å¸§é€Ÿåº¦åˆ†ææŠ˜çº¿å›¾"""
        if not self.detailed_results or len(self.detailed_results) < 10:
            print("æ•°æ®ä¸è¶³ï¼Œè·³è¿‡è¯¦ç»†æ—¶é—´æŠ˜çº¿å›¾ç”Ÿæˆ")
            return None
            
        try:
            print("æ­£åœ¨ç”Ÿæˆè¯¦ç»†é€Ÿåº¦åˆ†ææŠ˜çº¿å›¾...")
            
            # æå–æ•°æ®å¹¶è®¡ç®—é€Ÿåº¦æŒ‡æ ‡
            sample_ids = [r['sample_id'] for r in self.detailed_results]
            total_times = [r['total_time'] for r in self.detailed_results]
            inf_times = [r['inference_time'] for r in self.detailed_results]
            prep_times = [r['preprocessing_time'] for r in self.detailed_results]
            
            # è®¡ç®—FPSï¼ˆæ¯ç§’å¸§æ•°ï¼‰- é¿å…é™¤é›¶é”™è¯¯
            fps_total = []
            fps_inference = []
            throughput = []
            
            for i, (total_time, inf_time) in enumerate(zip(total_times, inf_times)):
                # ç¡®ä¿æ—¶é—´å€¼åˆç†ï¼Œé¿å…æ— ç©·å¤§
                total_time = max(total_time, 0.001)  # æœ€å°0.001ms
                inf_time = max(inf_time, 0.001)
                
                # FPS = 1000 / time_ms ï¼ˆä»æ¯«ç§’è½¬æ¢ï¼‰
                fps_t = min(1000.0 / total_time, 10000)  # é™åˆ¶æœ€å¤§FPSä¸º10000
                fps_i = min(1000.0 / inf_time, 10000)
                
                fps_total.append(fps_t)
                fps_inference.append(fps_i)
                
                # ååé‡ï¼ˆæ ·æœ¬/ç§’ï¼‰
                throughput.append(fps_t)
            
            # è®¡ç®—å¹³å‡å€¼
            avg_fps_total = np.mean(fps_total)
            avg_fps_inference = np.mean(fps_inference)
            avg_throughput = np.mean(throughput)
            
            # åˆ›å»ºå›¾è¡¨
            try:
                plt.style.use('seaborn-v0_8')
            except:
                try:
                    plt.style.use('seaborn')
                except:
                    plt.style.use('default')
            
            fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(16, 12))
            fig.suptitle(f'Per-Frame Speed Analysis: {self.model_info["name"]} on {self.dataset_name}', fontsize=16)
            
            # ä¸Šå›¾ï¼šFPSæ€§èƒ½å›¾
            ax1.plot(sample_ids, fps_total, label='Total FPS', color='blue', alpha=0.7, linewidth=1.5)
            ax1.plot(sample_ids, fps_inference, label='Inference FPS', color='red', alpha=0.7, linewidth=1.5)
            
            # æ·»åŠ å¹³å‡å€¼çº¿
            ax1.axhline(y=avg_fps_total, color='blue', linestyle='--', alpha=0.8, linewidth=2, 
                       label=f'Avg Total FPS: {avg_fps_total:.1f}')
            ax1.axhline(y=avg_fps_inference, color='red', linestyle='--', alpha=0.8, linewidth=2,
                       label=f'Avg Inference FPS: {avg_fps_inference:.1f}')
            
            ax1.set_xlabel('Sample/Image ID')
            ax1.set_ylabel('FPS (Frames Per Second)')
            ax1.set_title('Processing Speed per Frame')
            ax1.legend()
            ax1.grid(True, alpha=0.3)
            
            # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯æ–‡æœ¬
            stats_text = f"""Speed Statistics:
Total FPS: {avg_fps_total:.1f} Â± {np.std(fps_total):.1f}
Inference FPS: {avg_fps_inference:.1f} Â± {np.std(fps_inference):.1f}
Min FPS: {np.min(fps_total):.1f}
Max FPS: {np.max(fps_total):.1f}"""
            ax1.text(0.02, 0.98, stats_text, transform=ax1.transAxes, fontsize=10,
                    verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))
            
            # ä¸‹å›¾ï¼šååé‡å’Œæ€§èƒ½ç¨³å®šæ€§
            ax2.plot(sample_ids, throughput, label='Throughput (samples/sec)', color='green', alpha=0.6, linewidth=1)
            
            # è®¡ç®—ç§»åŠ¨å¹³å‡ï¼ˆçª—å£å¤§å°ä¸º10ï¼‰
            if len(throughput) >= 10:
                window_size = min(10, len(throughput) // 5)
                moving_avg = []
                for i in range(len(throughput)):
                    start_idx = max(0, i - window_size // 2)
                    end_idx = min(len(throughput), i + window_size // 2 + 1)
                    moving_avg.append(np.mean(throughput[start_idx:end_idx]))
                
                ax2.plot(sample_ids, moving_avg, label=f'Moving Average (window={window_size})', 
                        color='orange', linewidth=2)
            
            # æ·»åŠ å¹³å‡ååé‡çº¿
            ax2.axhline(y=avg_throughput, color='green', linestyle='--', alpha=0.8, linewidth=2,
                       label=f'Avg Throughput: {avg_throughput:.1f} samples/sec')
            
            # æ€§èƒ½ç¨³å®šæ€§åŒºé—´ï¼ˆÂ±1æ ‡å‡†å·®ï¼‰
            std_throughput = np.std(throughput)
            ax2.fill_between(sample_ids, 
                           avg_throughput - std_throughput, 
                           avg_throughput + std_throughput, 
                           alpha=0.2, color='gray', label=f'Â±1Ïƒ: {std_throughput:.1f}')
            
            ax2.set_xlabel('Sample/Image ID')
            ax2.set_ylabel('Throughput (samples/sec)')
            ax2.set_title('Processing Throughput and Stability')
            ax2.legend()
            ax2.grid(True, alpha=0.3)
            
            # æ€§èƒ½ç­‰çº§æ ‡è®°
            if self.model_type == 'classification':
                performance_levels = [
                    (100, 'Excellent', 'green'),
                    (50, 'Good', 'yellow'), 
                    (10, 'Fair', 'orange'),
                    (0, 'Slow', 'red')
                ]
            else:  # detection
                performance_levels = [
                    (30, 'Excellent', 'green'),
                    (15, 'Good', 'yellow'),
                    (5, 'Fair', 'orange'), 
                    (0, 'Slow', 'red')
                ]
            
            # æ·»åŠ æ€§èƒ½ç­‰çº§çº¿
            for level, label, color in performance_levels[:-1]:  # è·³è¿‡æœ€åçš„0çº¿
                ax2.axhline(y=level, color=color, linestyle=':', alpha=0.5, linewidth=1)
                ax2.text(max(sample_ids) * 0.95, level + std_throughput * 0.1, 
                        label, color=color, fontweight='bold', alpha=0.7)
            
            plt.tight_layout()
            
            # ä¿å­˜å›¾è¡¨
            timing_plot_filename = f"{hostname}_{self.model_type}_speed_analysis_{timestamp}.png"
            plt.savefig(timing_plot_filename, format='png', dpi=300, bbox_inches='tight')
            print(f"è¯¦ç»†é€Ÿåº¦åˆ†æå›¾è¡¨å·²ä¿å­˜è‡³: {timing_plot_filename}")
            
            plt.close(fig)
            return timing_plot_filename
            
        except Exception as e:
            print(f"åˆ›å»ºè¯¦ç»†é€Ÿåº¦æŠ˜çº¿å›¾æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return None

    def create_visualizations(self, stats, csv_filenames):
        """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
        try:
            print("æ­£åœ¨ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
            
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            hostname = socket.gethostname()
            
            # é¦–å…ˆç”Ÿæˆè¯¦ç»†æ—¶é—´æŠ˜çº¿å›¾
            timing_plot_file = self.create_detailed_timing_plot(timestamp, hostname)
            
            # è®¾ç½®å›¾è¡¨æ ·å¼ - ä¿®å¤æ ·å¼åç§°
            try:
                plt.style.use('seaborn-v0_8')
            except:
                try:
                    plt.style.use('seaborn')
                except:
                    plt.style.use('default')
                    
            fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
            fig.suptitle(f'Benchmark Results Summary: {stats["system_info"]["model_name"]} on {stats["system_info"]["dataset"]}', fontsize=16)
            
            # 1. æ—¶é—´åˆ†è§£é¥¼å›¾
            if stats['timing']:
                timing_data = [(k.replace('_', ' ').title(), v['avg']) for k, v in stats['timing'].items()]
                labels, values = zip(*timing_data)
                ax1.pie(values, labels=labels, autopct='%1.1f%%', startangle=90)
                ax1.set_title('Timing Breakdown')
            else:
                ax1.text(0.5, 0.5, 'No timing data available', ha='center', va='center', transform=ax1.transAxes)
                ax1.set_title('Timing Breakdown')
            
            # 2. èµ„æºåˆ©ç”¨ç‡æŸ±çŠ¶å›¾
            resources = ['CPU', 'Memory']
            usage = [stats['resources']['cpu']['avg'], stats['resources']['memory']['avg']]
            errors = [stats['resources']['cpu']['std'], stats['resources']['memory']['std']]
            
            if 'gpu' in stats['resources']:
                resources.extend(['GPU Memory', 'GPU Util'])
                usage.extend([stats['resources']['gpu']['memory']['avg'], 
                             stats['resources']['gpu']['utilization']['avg']])
                errors.extend([stats['resources']['gpu']['memory']['std'],
                              stats['resources']['gpu']['utilization']['std']])
            
            bars = ax2.bar(resources, usage, yerr=errors, capsize=5,
                          color=['skyblue', 'lightgreen', 'orange', 'red'][:len(resources)])
            ax2.set_title('Resource Utilization (with std dev)')
            ax2.set_ylabel('Usage (%)')
            ax2.set_ylim(0, 100)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, val, err in zip(bars, usage, errors):
                ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + err + 1, 
                        f'{val:.1f}%', ha='center', va='bottom')
            
            # 3. æ€§èƒ½æ—¶é—´åˆ†å¸ƒ - å¦‚æœæœ‰è¯¦ç»†ç»“æœçš„è¯
            if self.detailed_results and len(self.detailed_results) > 10:
                # ç»˜åˆ¶æ—¶é—´åˆ†å¸ƒç›´æ–¹å›¾
                total_times = [r['total_time'] for r in self.detailed_results]
                ax3.hist(total_times, bins=20, alpha=0.7, color='lightcoral', edgecolor='black')
                ax3.set_xlabel('Total Time per Sample (ms)')
                ax3.set_ylabel('Frequency')
                ax3.set_title('Processing Time Distribution')
                ax3.axvline(np.mean(total_times), color='red', linestyle='--', 
                           label=f'Mean: {np.mean(total_times):.2f}ms')
                ax3.legend()
            else:
                ax3.text(0.5, 0.5, 'Insufficient data\nfor distribution plot', 
                        ha='center', va='center', transform=ax3.transAxes)
                ax3.set_title('Processing Time Distribution')
            
            # 4. ç³»ç»Ÿä¿¡æ¯å’Œæ€§èƒ½æ€»ç»“
            if 'gpu' in stats['resources']:
                system_text = f"""System Information:
Device: {stats['system_info']['device']}
Model: {stats['system_info']['model_name']}
Dataset: {stats['system_info']['dataset']}

Performance Summary:
Throughput: {stats['performance']['throughput']:.2f} samples/sec
Avg time/sample: {stats['performance']['avg_time_per_sample']:.2f}ms
Total samples: {stats['performance']['total_samples']}

Resource Usage (Avg Â± Std):
CPU: {stats['resources']['cpu']['avg']:.1f}% Â± {stats['resources']['cpu']['std']:.1f}%
Memory: {stats['resources']['memory']['avg']:.1f}% Â± {stats['resources']['memory']['std']:.1f}%
GPU Mem: {stats['resources']['gpu']['memory']['avg']:.1f}% Â± {stats['resources']['gpu']['memory']['std']:.1f}%
GPU Util: {stats['resources']['gpu']['utilization']['avg']:.1f}% Â± {stats['resources']['gpu']['utilization']['std']:.1f}%"""
            else:
                system_text = f"""System Information:
Device: {stats['system_info']['device']}
Model: {stats['system_info']['model_name']}
Dataset: {stats['system_info']['dataset']}

Performance Summary:
Throughput: {stats['performance']['throughput']:.2f} samples/sec
Avg time/sample: {stats['performance']['avg_time_per_sample']:.2f}ms
Total samples: {stats['performance']['total_samples']}

Resource Usage (Avg Â± Std):
CPU: {stats['resources']['cpu']['avg']:.1f}% Â± {stats['resources']['cpu']['std']:.1f}%
Memory: {stats['resources']['memory']['avg']:.1f}% Â± {stats['resources']['memory']['std']:.1f}%"""
            
            ax4.text(0.05, 0.95, system_text, transform=ax4.transAxes, fontsize=9,
                    verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
            ax4.set_xlim(0, 1)
            ax4.set_ylim(0, 1)
            ax4.axis('off')
            ax4.set_title('System Info & Performance Summary')
            
            plt.tight_layout()
            
            # ä¿å­˜æ€»ç»“å›¾è¡¨
            summary_plot_filename = f"{hostname}_{self.model_type}_summary_{timestamp}.png"
            plt.savefig(summary_plot_filename, format='png', dpi=300, bbox_inches='tight')
            print(f"æ€§èƒ½æ€»ç»“å›¾è¡¨å·²ä¿å­˜è‡³: {summary_plot_filename}")
            
            # å…³é—­å›¾å½¢ä»¥é‡Šæ”¾å†…å­˜
            plt.close(fig)
            
            # è¿”å›ç”Ÿæˆçš„å›¾è¡¨æ–‡ä»¶å
            generated_plots = [summary_plot_filename]
            if timing_plot_file:
                generated_plots.append(timing_plot_file)
            
            return generated_plots
            
        except Exception as e:
            print(f"åˆ›å»ºå¯è§†åŒ–æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return []


def main():
    """ä¸»å‡½æ•°"""
    # æ£€æŸ¥ä¾èµ–
    missing_deps = []
    
    if not ULTRALYTICS_AVAILABLE:
        missing_deps.append("ultralytics (pip install ultralytics)")
    
    if not NVML_AVAILABLE:
        missing_deps.append("nvidia-ml-py3 (pip install nvidia-ml-py3)")
    
    try:
        import timm
    except ImportError:
        missing_deps.append("timm (pip install timm)")
    
    try:
        import matplotlib.pyplot as plt
        import seaborn as sns
    except ImportError:
        missing_deps.append("matplotlib seaborn (pip install matplotlib seaborn)")
    
    if missing_deps:
        print("å»ºè®®å®‰è£…ä»¥ä¸‹ä¾èµ–ä»¥è·å¾—å®Œæ•´åŠŸèƒ½:")
        for dep in missing_deps:
            print(f"  - {dep}")
        print()
    
    # åˆ›å»ºåŸºå‡†æµ‹è¯•å·¥å…·
    benchmark = InteractiveBenchmark()
    
    # äº¤äº’å¼è®¾ç½®
    benchmark.interactive_setup()
    
    # åŠ è½½æ•°æ®é›†
    benchmark.load_dataset_func()
    
    # åŠ è½½æ¨¡å‹
    benchmark.load_model()
    
    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    stats = benchmark.run_benchmark()
    
    if stats:
        # æ‰“å°ç®€æ´ç»“æœ
        benchmark.print_concise_results(stats)
        
        # ä¿å­˜è¯¦ç»†CSVç»“æœ
        csv_filenames = benchmark.save_detailed_csv_results(stats)
        
        # åˆ›å»ºå¯è§†åŒ–
        plot_files = benchmark.create_visualizations(stats, csv_filenames)
        
        print("\næµ‹è¯•å®Œæˆ!")
        print(f"è¯¦ç»†ç»“æœæ–‡ä»¶: {csv_filenames[0]}")
        print(f"æ±‡æ€»ç»“æœæ–‡ä»¶: {csv_filenames[1]}")
        if plot_files:
            print("ç”Ÿæˆçš„å›¾è¡¨æ–‡ä»¶:")
            for plot_file in plot_files:
                print(f"  - {plot_file}")
    else:
        print("æµ‹è¯•å¤±è´¥")
        sys.exit(1)


if __name__ == "__main__":
    main()
