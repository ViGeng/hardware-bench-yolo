#!/usr/bin/env python3
"""
YOLOv8 æ€§èƒ½åŸºå‡†æµ‹è¯•å·¥å…· - æ”¹è¿›ç‰ˆ
ä¸»è¦æ”¹è¿›ï¼š
1. æ”¯æŒå‘½ä»¤è¡Œå‚æ•°ï¼Œä¸ç”¨ä¿®æ”¹ä»£ç 
2. æ”¯æŒå¤šç§è¾“å‡ºæ ¼å¼ï¼ˆtxt, json, csvï¼‰
3. æ›´å¥½çš„é”™è¯¯å¤„ç†
4. è‡ªåŠ¨æ£€æµ‹æ–‡ä»¶å’Œè®¾å¤‡
5. å¯é€‰æ˜¾ç¤ºæ£€æµ‹ç»“æžœï¼ˆåƒåŽŸç‰ˆä¸€æ ·ï¼‰
6. å¯é€‰æ˜¾ç¤ºè¿›åº¦ä¿¡æ¯
7. å®Œæ•´çš„å¯è§†åŒ–åŠŸèƒ½ï¼ˆè‹±æ–‡æ ‡ç­¾ï¼Œæ¸…æ™°å›¾è¡¨ï¼‰
"""

import argparse
import json
import csv
from ultralytics import YOLO
import numpy as np
import time
import psutil
import torch
import threading
import socket
from collections import deque
import os
import sys
from pathlib import Path

try:
    import pynvml
    NVML_AVAILABLE = True
except ImportError:
    NVML_AVAILABLE = False
    print("Warning: pynvml not available, GPU monitoring disabled")

try:
    import matplotlib.pyplot as plt
    import seaborn as sns
    VISUALIZATION_AVAILABLE = True
    # è®¾ç½®å­—ä½“å’Œæ ·å¼ - ä½¿ç”¨è‹±æ–‡é¿å…ä¸­æ–‡å­—ä½“é—®é¢˜
    plt.rcParams['font.family'] = ['Arial', 'DejaVu Sans', 'Liberation Sans']
    plt.rcParams['axes.unicode_minus'] = False
    plt.rcParams['figure.autolayout'] = True
    sns.set_style("whitegrid")
    sns.set_palette("husl")
except ImportError:
    VISUALIZATION_AVAILABLE = False
    print("Warning: matplotlib/seaborn not available, visualization disabled")

def parse_arguments():
    """è§£æžå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='YOLOv8 Performance Benchmark Tool')
    
    # ä¸»è¦å‚æ•°
    parser.add_argument('--model', '-m', default='yolov8n.pt',
                        help='æ¨¡åž‹è·¯å¾„æˆ–åç§° (é»˜è®¤: yolov8n.pt)')
    parser.add_argument('--source', '-s', default='0',
                        help='è§†é¢‘æº: æ–‡ä»¶è·¯å¾„æˆ–æ‘„åƒå¤´è®¾å¤‡å· (é»˜è®¤: 0)')
    
    # å¯é€‰å‚æ•°
    parser.add_argument('--device', '-d', default='auto',
                        help='è¿è¡Œè®¾å¤‡: cpu, cuda:0, auto (é»˜è®¤: auto)')
    parser.add_argument('--batch-size', '-b', type=int, default=1,
                        help='æ‰¹æ¬¡å¤§å° (é»˜è®¤: 1)')
    parser.add_argument('--max-frames', '-f', type=int,
                        help='æœ€å¤§å¤„ç†å¸§æ•° (é»˜è®¤: æ— é™åˆ¶)')
    parser.add_argument('--imgsz', type=int,
                        help='è¾“å…¥å›¾åƒå°ºå¯¸ (é»˜è®¤: æ¨¡åž‹é»˜è®¤å€¼)')
    
    # è¾“å‡ºé€‰é¡¹
    parser.add_argument('--output-format', '-o', nargs='+', 
                        choices=['txt', 'json', 'csv'], default=['txt'],
                        help='è¾“å‡ºæ ¼å¼ (é»˜è®¤: txt)')
    parser.add_argument('--verbose', '-v', action='store_true',
                        help='æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯')
    parser.add_argument('--show-detections', action='store_true',
                        help='æ˜¾ç¤ºæ¯å¸§æ£€æµ‹ç»“æžœ')
    parser.add_argument('--show-progress', action='store_true',
                        help='æ˜¾ç¤ºå¤„ç†è¿›åº¦ï¼ˆæ¯10å¸§ï¼‰')
    parser.add_argument('--plot', action='store_true',
                        help='ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨')
    parser.add_argument('--plot-realtime', action='store_true',
                        help='å®žæ—¶æ˜¾ç¤ºæ€§èƒ½æ›²çº¿')
    
    return parser.parse_args()

class BenchmarkTool:
    def __init__(self, args):
        self.args = args
        self.model = None
        self.monitoring = True
        
        # ç³»ç»Ÿä¿¡æ¯
        self.cpu_count = psutil.cpu_count()
        self.cpu_logical_count = psutil.cpu_count(logical=True)
        
        # åˆå§‹åŒ–æŒ‡æ ‡æ”¶é›†
        self.preprocess_times = []
        self.inference_times = []
        self.postprocess_times = []
        self.total_frames = 0
        self.start_time = None
        
        # åˆå§‹åŒ–èµ„æºç›‘æŽ§
        self.cpu_percentages = deque(maxlen=1000)
        self.memory_usages = deque(maxlen=1000)
        self.gpu_mem_usages = deque(maxlen=1000) if torch.cuda.is_available() else None
        self.gpu_util_usages = deque(maxlen=1000) if torch.cuda.is_available() else None
        
        # å¯è§†åŒ–æ•°æ®æ”¶é›†
        self.frame_timestamps = []
        self.frame_fps = []
        self.frame_inference_times = []
        self.resource_timestamps = []
        
        self.monitor_thread = None
        self.nvml_handle = None
    
    def print_detection_results(self, result, frame_num):
        """æ˜¾ç¤ºæ£€æµ‹ç»“æžœ"""
        try:
            if result.boxes is not None and len(result.boxes) > 0:
                # èŽ·å–æ£€æµ‹åˆ°çš„ç±»åˆ«
                class_ids = result.boxes.cls.cpu().numpy().astype(int)
                confidences = result.boxes.conf.cpu().numpy()
                
                # ç»Ÿè®¡æ¯ç§ç‰©å“çš„æ•°é‡
                detections = {}
                for cls_id, conf in zip(class_ids, confidences):
                    class_name = result.names[cls_id]
                    if class_name not in detections:
                        detections[class_name] = []
                    detections[class_name].append(conf)
                
                # æ ¼å¼åŒ–è¾“å‡º
                if detections:
                    detection_strs = []
                    for class_name, confs in detections.items():
                        count = len(confs)
                        avg_conf = sum(confs) / count
                        if count == 1:
                            detection_strs.append(f"{class_name}({avg_conf:.2f})")
                        else:
                            detection_strs.append(f"{class_name}x{count}({avg_conf:.2f})")
                    
                    print(f"Frame {frame_num}: {', '.join(detection_strs)}")
                else:
                    print(f"Frame {frame_num}: No objects detected")
            else:
                print(f"Frame {frame_num}: No objects detected")
                
        except Exception as e:
            if self.args.verbose:
                print(f"Frame {frame_num}: Detection display error: {e}")
            else:
                print(f"Frame {frame_num}: -")
        
    def load_model(self):
        """åŠ è½½YOLOæ¨¡åž‹"""
        print(f"æ­£åœ¨åŠ è½½æ¨¡åž‹: {self.args.model}")
        try:
            self.model = YOLO(self.args.model)
            print(f"æ¨¡åž‹åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            print(f"æ¨¡åž‹åŠ è½½å¤±è´¥: {e}")
            sys.exit(1)
    
    def validate_source(self):
        """éªŒè¯è¾“å…¥æº"""
        if self.args.source.isdigit():
            print(f"ä½¿ç”¨æ‘„åƒå¤´è®¾å¤‡: {self.args.source}")
            return int(self.args.source)
        elif os.path.exists(self.args.source):
            print(f"ä½¿ç”¨è§†é¢‘æ–‡ä»¶: {self.args.source}")
            return self.args.source
        else:
            print(f"é”™è¯¯: æ‰¾ä¸åˆ°è¾“å…¥æº '{self.args.source}'")
            sys.exit(1)
    
    def check_and_fix_device(self):
        """æ£€æŸ¥å¹¶ä¿®å¤è®¾å¤‡è®¾ç½®"""
        if self.args.device == 'auto':
            if torch.cuda.is_available() and torch.cuda.device_count() > 0:
                # æµ‹è¯•CUDAæ˜¯å¦çœŸçš„å¯ç”¨
                try:
                    test_tensor = torch.tensor([1.0]).cuda()
                    self.args.device = 'cuda:0'
                    print(f"è‡ªåŠ¨é€‰æ‹©è®¾å¤‡: cuda:0")
                except Exception as e:
                    print(f"CUDAä¸å¯ç”¨ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°CPU: {e}")
                    self.args.device = 'cpu'
            else:
                print("æœªæ£€æµ‹åˆ°å¯ç”¨çš„CUDAè®¾å¤‡ï¼Œä½¿ç”¨CPU")
                self.args.device = 'cpu'
        
        # éªŒè¯è®¾å¤‡
        if 'cuda' in self.args.device:
            try:
                test_tensor = torch.tensor([1.0]).to(self.args.device)
                print(f"è®¾å¤‡éªŒè¯æˆåŠŸ: {self.args.device}")
            except Exception as e:
                print(f"è®¾å¤‡ {self.args.device} ä¸å¯ç”¨ï¼Œåˆ‡æ¢åˆ°CPU: {e}")
                self.args.device = 'cpu'

    def monitor_resources(self):
        """èµ„æºç›‘æŽ§å‡½æ•°"""
        # åˆå§‹åŒ–GPUç›‘æŽ§
        if torch.cuda.is_available() and NVML_AVAILABLE:
            try:
                pynvml.nvmlInit()
                self.nvml_handle = pynvml.nvmlDeviceGetHandleByIndex(0)
                if self.args.verbose:
                    print("GPUç›‘æŽ§å·²åˆå§‹åŒ–")
            except Exception as e:
                if self.args.verbose:
                    print(f"GPUç›‘æŽ§åˆå§‹åŒ–å¤±è´¥: {e}")
                self.nvml_handle = None
        
        # èŽ·å–å½“å‰è¿›ç¨‹
        current_process = psutil.Process()
        
        while self.monitoring:
            try:
                current_time = time.time()
                
                # CPUä½¿ç”¨çŽ‡
                self.cpu_percentages.append(current_process.cpu_percent(interval=0.1))
                
                # å†…å­˜ä½¿ç”¨çŽ‡
                memory = psutil.virtual_memory()
                self.memory_usages.append(memory.percent)
                
                # è®°å½•æ—¶é—´æˆ³ç”¨äºŽå¯è§†åŒ–
                if self.start_time:
                    self.resource_timestamps.append(current_time - self.start_time)
                
                # GPUä½¿ç”¨çŽ‡
                if torch.cuda.is_available():
                    # GPUå†…å­˜ä½¿ç”¨çŽ‡
                    gpu_mem_alloc = torch.cuda.memory_allocated(0)
                    gpu_mem_total = torch.cuda.get_device_properties(0).total_memory
                    gpu_mem_percent = (gpu_mem_alloc / gpu_mem_total) * 100
                    self.gpu_mem_usages.append(gpu_mem_percent)
                    
                    # GPUåˆ©ç”¨çŽ‡
                    if self.nvml_handle is not None:
                        try:
                            utilization = pynvml.nvmlDeviceGetUtilizationRates(self.nvml_handle)
                            self.gpu_util_usages.append(utilization.gpu)
                        except Exception:
                            last_value = self.gpu_util_usages[-1] if self.gpu_util_usages else 0
                            self.gpu_util_usages.append(last_value)
                
                time.sleep(0.1)
                
            except Exception as e:
                if self.args.verbose:
                    print(f"ç›‘æŽ§é”™è¯¯: {e}")
                break
        
        # æ¸…ç†
        if torch.cuda.is_available() and self.nvml_handle is not None and NVML_AVAILABLE:
            try:
                pynvml.nvmlShutdown()
            except:
                pass
    
    def run_benchmark(self):
        """è¿è¡ŒåŸºå‡†æµ‹è¯•"""
        print("å¼€å§‹åŸºå‡†æµ‹è¯•...")
        
        # æ£€æŸ¥å¹¶ä¿®å¤è®¾å¤‡è®¾ç½®
        self.check_and_fix_device()
        
        # å¯åŠ¨èµ„æºç›‘æŽ§çº¿ç¨‹
        self.monitor_thread = threading.Thread(target=self.monitor_resources)
        self.monitor_thread.daemon = True
        self.monitor_thread.start()
        
        # éªŒè¯è¾“å…¥æº
        source = self.validate_source()
        
        # è®¾ç½®æŽ¨ç†å‚æ•°
        inference_params = {
            'stream': True,
            'batch': self.args.batch_size,
            'device': self.args.device,
            'verbose': False  # é¿å…å¹²æ‰°è¾“å‡º
        }
        
        if self.args.imgsz:
            inference_params['imgsz'] = self.args.imgsz
        
        if self.args.verbose:
            print(f"æŽ¨ç†å‚æ•°: {inference_params}")
            print(f"CPUé…ç½®: {self.cpu_count}æ ¸å¿ƒ/{self.cpu_logical_count}çº¿ç¨‹ (æœ€å¤§CPUä½¿ç”¨çŽ‡: {self.cpu_logical_count * 100}%)")
        
        self.start_time = time.time()
        
        # ç”¨äºŽè®¡ç®—çž¬æ—¶FPS
        last_time = self.start_time
        last_frame_count = 0
        
        try:
            # è¿è¡ŒæŽ¨ç†
            results = self.model(source, **inference_params)
            
            # å¤„ç†ç»“æžœ
            for result in results:
                # æ£€æŸ¥å¸§æ•°é™åˆ¶ï¼ˆå¦‚æžœä¸æƒ³é™åˆ¶å¸§æ•°ï¼Œå¯ä»¥æ³¨é‡ŠæŽ‰ä¸‹é¢å‡ è¡Œï¼‰
                if self.args.max_frames and self.total_frames >= self.args.max_frames:
                    if self.args.verbose:
                        print(f"è¾¾åˆ°æœ€å¤§å¸§æ•°é™åˆ¶: {self.args.max_frames}")
                    break
                
                # æ”¶é›†æŒ‡æ ‡
                speed_data = result.speed
                self.preprocess_times.append(speed_data.get("preprocess", 0))
                self.inference_times.append(speed_data.get("inference", 0))
                self.postprocess_times.append(speed_data.get("postprocess", 0))
                self.total_frames += 1
                
                # æ”¶é›†å¯è§†åŒ–æ•°æ®
                current_time = time.time()
                self.frame_timestamps.append(current_time - self.start_time)
                
                # è®¡ç®—å½“å‰FPS
                if len(self.frame_timestamps) >= 2:
                    time_diff = self.frame_timestamps[-1] - self.frame_timestamps[-2]
                    current_fps = 1.0 / time_diff if time_diff > 0 else 0
                else:
                    current_fps = 0
                self.frame_fps.append(current_fps)
                self.frame_inference_times.append(speed_data.get("inference", 0))
                
                # æ˜¾ç¤ºæ£€æµ‹ç»“æžœï¼ˆå¦‚æžœå¯ç”¨ï¼‰
                if self.args.show_detections:
                    self.print_detection_results(result, self.total_frames)
                
                # æ˜¾ç¤ºè¿›åº¦ï¼ˆæ˜¾ç¤ºç´¯ç§¯å¹³å‡FPSå’Œçž¬æ—¶FPSï¼‰
                if (self.args.show_progress or self.args.verbose) and self.total_frames % 10 == 0:
                    current_time = time.time()
                    
                    # ç´¯ç§¯å¹³å‡FPS
                    avg_fps = self.total_frames / (current_time - self.start_time)
                    
                    # çž¬æ—¶FPSï¼ˆæœ€è¿‘10å¸§çš„å¹³å‡ï¼‰
                    instant_fps = 10 / (current_time - last_time) if current_time > last_time else 0
                    
                    print(f"å·²å¤„ç† {self.total_frames} å¸§ | å¹³å‡FPS: {avg_fps:.2f} | çž¬æ—¶FPS: {instant_fps:.2f}")
                    
                    last_time = current_time
                    last_frame_count = self.total_frames
                
        except KeyboardInterrupt:
            print("\næµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        except Exception as e:
            print(f"æŽ¨ç†è¿‡ç¨‹å‡ºé”™: {e}")
            return False
        
        # åœæ­¢ç›‘æŽ§
        self.monitoring = False
        if self.monitor_thread and self.monitor_thread.is_alive():
            self.monitor_thread.join(timeout=1.0)
        
        if self.total_frames == 0:
            print("æ²¡æœ‰å¤„ç†ä»»ä½•å¸§")
            return False
        
        print(f"æµ‹è¯•å®Œæˆï¼Œå…±å¤„ç† {self.total_frames} å¸§")
        return True
    
    def calculate_statistics(self):
        """è®¡ç®—ç»Ÿè®¡æ•°æ®"""
        if not self.preprocess_times:
            return None
        
        end_time = time.time()
        total_time = end_time - self.start_time
        throughput = self.total_frames / total_time if total_time > 0 else 0
        
        total_per_frame = [p + i + pp for p, i, pp in zip(
            self.preprocess_times, self.inference_times, self.postprocess_times)]
        
        stats = {
            'summary': {
                'total_frames': self.total_frames,
                'total_time': total_time,
                'throughput': throughput,
                'avg_frame_time': np.mean(total_per_frame)
            },
            'timing': {
                'preprocess': {
                    'min': np.min(self.preprocess_times),
                    'max': np.max(self.preprocess_times),
                    'avg': np.mean(self.preprocess_times),
                    'std': np.std(self.preprocess_times)
                },
                'inference': {
                    'min': np.min(self.inference_times),
                    'max': np.max(self.inference_times),
                    'avg': np.mean(self.inference_times),
                    'std': np.std(self.inference_times)
                },
                'postprocess': {
                    'min': np.min(self.postprocess_times),
                    'max': np.max(self.postprocess_times),
                    'avg': np.mean(self.postprocess_times),
                    'std': np.std(self.postprocess_times)
                },
                'total_per_frame': {
                    'min': np.min(total_per_frame),
                    'max': np.max(total_per_frame),
                    'avg': np.mean(total_per_frame),
                    'std': np.std(total_per_frame)
                }
            },
            'resources': {
                'cpu': {
                    'min': np.min(self.cpu_percentages) if self.cpu_percentages else 0,
                    'max': np.max(self.cpu_percentages) if self.cpu_percentages else 0,
                    'avg': np.mean(self.cpu_percentages) if self.cpu_percentages else 0
                },
                'memory': {
                    'min': np.min(self.memory_usages) if self.memory_usages else 0,
                    'max': np.max(self.memory_usages) if self.memory_usages else 0,
                    'avg': np.mean(self.memory_usages) if self.memory_usages else 0
                }
            },
            'system_info': {
                'hostname': socket.gethostname(),
                'model': self.args.model,
                'source': self.args.source,
                'device': self.args.device,
                'batch_size': self.args.batch_size,
                'torch_version': torch.__version__,
                'cuda_available': torch.cuda.is_available(),
                'device_name': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU',
                'cpu_cores': self.cpu_count,
                'cpu_threads': self.cpu_logical_count,
                'cpu_max_usage': f"{self.cpu_logical_count * 100}%"
            }
        }
        
        # æ·»åŠ GPUç»Ÿè®¡
        if torch.cuda.is_available() and self.gpu_mem_usages and self.gpu_util_usages:
            # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ•°æ®
            gpu_mem_data = list(self.gpu_mem_usages)
            gpu_util_data = list(self.gpu_util_usages)
            
            if len(gpu_mem_data) > 0 and len(gpu_util_data) > 0:
                stats['resources']['gpu'] = {
                    'memory': {
                        'min': float(np.min(gpu_mem_data)),
                        'max': float(np.max(gpu_mem_data)),
                        'avg': float(np.mean(gpu_mem_data))
                    },
                    'utilization': {
                        'min': float(np.min(gpu_util_data)),
                        'max': float(np.max(gpu_util_data)),
                        'avg': float(np.mean(gpu_util_data))
                    }
                }
            else:
                # å¦‚æžœæ•°æ®ä¸è¶³ï¼Œè®¾ç½®é»˜è®¤å€¼
                stats['resources']['gpu'] = {
                    'memory': {'min': 0.0, 'max': 0.0, 'avg': 0.0},
                    'utilization': {'min': 0.0, 'max': 0.0, 'avg': 0.0}
                }
        else:
            # å¦‚æžœGPUä¸å¯ç”¨æˆ–æ²¡æœ‰æ•°æ®ï¼Œä¸æ·»åŠ GPUç»Ÿè®¡æˆ–è®¾ç½®ä¸ºNone
            if self.args.verbose:
                print("   GPUæ•°æ®ä¸å¯ç”¨ï¼Œè·³è¿‡GPUç»Ÿè®¡")
        
        return stats
    
    def create_visualizations(self, stats, save_path=None):
        """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
        if not VISUALIZATION_AVAILABLE:
            print("âŒ matplotlib/seabornä¸å¯ç”¨ï¼Œè·³è¿‡å¯è§†åŒ–")
            return
        
        if not self.frame_timestamps:
            print("âŒ æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®ç”¨äºŽå¯è§†åŒ–")
            return
        
        try:
            print("ðŸ“Š æ­£åœ¨ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
            
            # è®¾ç½®matplotlibåŽç«¯
            import matplotlib
            matplotlib.use('Agg')  # å¼ºåˆ¶ä½¿ç”¨AggåŽç«¯ï¼Œç¡®ä¿å¯ä»¥ä¿å­˜å›¾ç‰‡
            
            # åˆ›å»ºå›¾è¡¨
            fig = plt.figure(figsize=(16, 12))
            
            print("   - ç”ŸæˆFPSæ›²çº¿...")
            # 1. æ€§èƒ½æ›²çº¿å›¾
            plt.subplot(2, 3, 1)
            plt.plot(self.frame_timestamps, self.frame_fps, 'b-', linewidth=2, alpha=0.8, label='Actual FPS')
            plt.title('Real-time FPS Performance', fontsize=14, fontweight='bold')
            plt.xlabel('Time (seconds)')
            plt.ylabel('Frames Per Second (FPS)')
            plt.grid(True, alpha=0.3)
            avg_fps = np.mean(self.frame_fps[10:]) if len(self.frame_fps) > 10 else np.mean(self.frame_fps)
            plt.axhline(y=avg_fps, color='r', linestyle='--', alpha=0.8, 
                       label=f'Average: {avg_fps:.2f} FPS')
            
            # æ·»åŠ æ€§èƒ½åŒºé—´å‚è€ƒçº¿
            plt.axhline(y=30, color='g', linestyle=':', alpha=0.5, label='Excellent (30+ FPS)')
            plt.axhline(y=15, color='orange', linestyle=':', alpha=0.5, label='Good (15+ FPS)')
            plt.axhline(y=5, color='red', linestyle=':', alpha=0.5, label='Minimum (5+ FPS)')
            
            plt.legend(fontsize=8)
            plt.ylim(bottom=0)
            
            print("   - ç”Ÿæˆæ—¶é—´åˆ†å¸ƒå›¾...")
            # 2. æŽ¨ç†æ—¶é—´åˆ†å¸ƒ
            plt.subplot(2, 3, 2)
            inference_times = self.frame_inference_times
            n_bins = min(30, max(10, len(inference_times) // 5))
            plt.hist(inference_times, bins=n_bins, alpha=0.7, color='skyblue', 
                    edgecolor='black', density=False)
            plt.title('Inference Time Distribution', fontsize=14, fontweight='bold')
            plt.xlabel('Inference Time (milliseconds)')
            plt.ylabel('Number of Frames')
            
            mean_time = np.mean(inference_times)
            std_time = np.std(inference_times)
            plt.axvline(x=mean_time, color='r', linestyle='--', 
                       label=f'Mean: {mean_time:.2f}ms')
            plt.axvline(x=mean_time + std_time, color='orange', linestyle=':', 
                       label=f'+1Ïƒ: {mean_time + std_time:.2f}ms')
            plt.axvline(x=mean_time - std_time, color='orange', linestyle=':', 
                       label=f'-1Ïƒ: {mean_time - std_time:.2f}ms')
            plt.legend(fontsize=8)
            
            print("   - ç”Ÿæˆæ—¶é—´åˆ†è§£å›¾...")
            # 3. å¤„ç†æ—¶é—´åˆ†è§£
            plt.subplot(2, 3, 3)
            stages = ['Preprocess', 'Inference', 'Postprocess']
            times = [
                np.mean(self.preprocess_times),
                np.mean(self.inference_times), 
                np.mean(self.postprocess_times)
            ]
            colors = ['lightcoral', 'skyblue', 'lightgreen']
            bars = plt.bar(stages, times, color=colors, alpha=0.8, edgecolor='black', linewidth=1)
            plt.title('Processing Time Breakdown', fontsize=14, fontweight='bold')
            plt.ylabel('Time (milliseconds)')
            plt.xlabel('Processing Stage')
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾å’Œç™¾åˆ†æ¯”
            total_time = sum(times)
            for bar, time_val in zip(bars, times):
                percentage = (time_val / total_time) * 100
                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(times)*0.02,
                        f'{time_val:.1f}ms\n({percentage:.1f}%)', 
                        ha='center', va='bottom', fontweight='bold', fontsize=9)
            
            plt.ylim(0, max(times) * 1.2)
            
            print("   - ç”Ÿæˆèµ„æºä½¿ç”¨å›¾...")
            # 4. èµ„æºä½¿ç”¨çŽ‡æ—¶é—´åºåˆ—
            plt.subplot(2, 3, 4)
            if self.cpu_percentages and self.resource_timestamps:
                cpu_data = list(self.cpu_percentages)
                time_data = self.resource_timestamps[:len(cpu_data)]
                plt.plot(time_data, cpu_data, 'g-', label='CPU Usage', linewidth=2, alpha=0.8)
                
                if self.gpu_util_usages and torch.cuda.is_available():
                    gpu_data = list(self.gpu_util_usages)[:len(time_data)]
                    plt.plot(time_data, gpu_data, 'r-', label='GPU Utilization', linewidth=2, alpha=0.8)
                
                plt.title('Resource Usage Over Time', fontsize=14, fontweight='bold')
                plt.xlabel('Time (seconds)')
                plt.ylabel('Usage Percentage (%)')
                plt.legend()
                plt.grid(True, alpha=0.3)
                plt.ylim(0, max(100, max(cpu_data) * 1.1) if cpu_data else 100)
            else:
                plt.text(0.5, 0.5, 'No Resource Data Available', 
                        ha='center', va='center', transform=plt.gca().transAxes,
                        fontsize=12, style='italic')
                plt.title('Resource Usage Over Time', fontsize=14, fontweight='bold')
            
            print("   - ç”Ÿæˆèµ„æºå¯¹æ¯”å›¾...")
            # 5. CPU vs GPUè´Ÿè½½å¯¹æ¯”
            plt.subplot(2, 3, 5)
            resource_labels = ['CPU Usage\n(%)', 'GPU Utilization\n(%)', 'Memory Usage\n(%)']
            
            # å®‰å…¨èŽ·å–èµ„æºæ•°æ®ï¼Œé¿å…KeyError
            cpu_avg = stats['resources']['cpu']['avg'] if 'cpu' in stats['resources'] and 'avg' in stats['resources']['cpu'] else 0
            gpu_avg = 0
            if 'gpu' in stats['resources'] and stats['resources']['gpu']:
                if 'utilization' in stats['resources']['gpu'] and 'avg' in stats['resources']['gpu']['utilization']:
                    gpu_avg = stats['resources']['gpu']['utilization']['avg']
            memory_avg = stats['resources']['memory']['avg'] if 'memory' in stats['resources'] and 'avg' in stats['resources']['memory'] else 0
            
            resource_values = [cpu_avg, gpu_avg, memory_avg]
            colors = ['lightcoral', 'skyblue', 'lightgreen']
            
            bars = plt.bar(resource_labels, resource_values, color=colors, alpha=0.8, 
                          edgecolor='black', linewidth=1)
            plt.title('Average Resource Utilization', fontsize=14, fontweight='bold')
            plt.ylabel('Utilization Percentage (%)')
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾å’Œæ€§èƒ½è¯„ä¼°
            for bar, value, label in zip(bars, resource_values, ['CPU', 'GPU', 'Memory']):
                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(resource_values)*0.02,
                        f'{value:.1f}%', ha='center', va='bottom', fontweight='bold', fontsize=10)
                
                # æ·»åŠ æ€§èƒ½è¯„ä¼°
                if label == 'CPU':
                    if value > 300:
                        status = 'High Load'
                    elif value > 100:
                        status = 'Multi-core'
                    else:
                        status = 'Normal'
                elif label == 'GPU':
                    if value > 80:
                        status = 'High Util'
                    elif value > 50:
                        status = 'Good Util'
                    else:
                        status = 'Low Util'
                else:  # Memory
                    if value > 80:
                        status = 'High Usage'
                    elif value > 50:
                        status = 'Moderate'
                    else:
                        status = 'Low Usage'
                
                plt.text(bar.get_x() + bar.get_width()/2, -max(resource_values)*0.08,
                        status, ha='center', va='top', fontsize=8, style='italic')
            
            plt.ylim(-max(resource_values)*0.1, max(resource_values)*1.15)
            
            print("   - ç”Ÿæˆæ€§èƒ½è¯„ä¼°å›¾...")
            # 6. æ€§èƒ½è¯„ä¼°é›·è¾¾å›¾
            try:
                plt.subplot(2, 3, 6, projection='polar')
                
                # å®‰å…¨èŽ·å–æ•°æ®ï¼Œé¿å…KeyError
                gpu_avg = 50  # é»˜è®¤å€¼
                if 'gpu' in stats['resources'] and stats['resources']['gpu']:
                    if 'utilization' in stats['resources']['gpu'] and 'avg' in stats['resources']['gpu']['utilization']:
                        gpu_avg = stats['resources']['gpu']['utilization']['avg']
                
                cpu_avg = stats['resources']['cpu']['avg'] if 'cpu' in stats['resources'] and 'avg' in stats['resources']['cpu'] else 100
                inference_avg = stats['timing']['inference']['avg'] if 'inference' in stats['timing'] and 'avg' in stats['timing']['inference'] else 50
                inference_std = stats['timing']['inference']['std'] if 'inference' in stats['timing'] and 'std' in stats['timing']['inference'] else 10
                throughput = stats['summary']['throughput'] if 'throughput' in stats['summary'] else 10
                
                # æ€§èƒ½æŒ‡æ ‡ (å½’ä¸€åŒ–åˆ°0-100)
                metrics = {
                    'FPS\nPerformance': min(throughput / 30 * 100, 100),  # 30fpsä¸ºæ»¡åˆ†
                    'Inference\nSpeed': max(0, 100 - inference_avg / 100 * 100),  # 100msä¸º0åˆ†
                    'CPU\nEfficiency': max(0, 100 - cpu_avg / 400 * 100),  # 400%ä¸º0åˆ†
                    'GPU\nUtilization': min(gpu_avg, 100),  # ç›´æŽ¥ä½¿ç”¨GPUåˆ©ç”¨çŽ‡
                    'Performance\nStability': max(0, 100 - inference_std / 50 * 100)  # 50msæ ‡å‡†å·®ä¸º0åˆ†
                }
                
                angles = np.linspace(0, 2*np.pi, len(metrics), endpoint=False)
                values = list(metrics.values())
                
                # é—­åˆå›¾å½¢
                angles = np.concatenate((angles, [angles[0]]))
                values.append(values[0])
                
                plt.plot(angles, values, 'o-', linewidth=2, color='blue', alpha=0.8, markersize=6)
                plt.fill(angles, values, alpha=0.25, color='blue')
                
                # è®¾ç½®æ ‡ç­¾
                labels = list(metrics.keys())
                plt.xticks(angles[:-1], labels, fontsize=9)
                
                # è®¾ç½®åˆ»åº¦å’Œç½‘æ ¼
                plt.ylim(0, 100)
                plt.yticks([20, 40, 60, 80, 100], ['20', '40', '60', '80', '100'], fontsize=8)
                plt.grid(True, alpha=0.3)
                
                # æ·»åŠ æ€§èƒ½è¯„çº§åœ†åœˆ
                for radius, color, alpha in [(80, 'green', 0.1), (60, 'yellow', 0.1), (40, 'orange', 0.1)]:
                    circle_angles = np.linspace(0, 2*np.pi, 100)
                    circle_values = [radius] * 100
                    plt.plot(circle_angles, circle_values, color=color, alpha=alpha, linewidth=1)
                
                plt.title('Performance Radar Chart\n(Higher = Better)', fontsize=12, fontweight='bold', pad=20)
                
                # æ·»åŠ å›¾ä¾‹è¯´æ˜Ž
                legend_text = f"Overall Score: {np.mean(values[:-1]):.1f}/100"
                plt.text(0.5, -0.15, legend_text, transform=plt.gca().transAxes, 
                        ha='center', va='center', fontsize=10, fontweight='bold',
                        bbox=dict(boxstyle="round,pad=0.3", facecolor="lightblue", alpha=0.7))
                
            except Exception as e:
                print(f"   - é›·è¾¾å›¾ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨æŸ±çŠ¶å›¾æ›¿ä»£: {e}")
                # å¦‚æžœé›·è¾¾å›¾å¤±è´¥ï¼Œåˆ›å»ºç®€å•çš„æŸ±çŠ¶å›¾æ›¿ä»£
                plt.subplot(2, 3, 6)
                
                # å®‰å…¨èŽ·å–æ•°æ®
                gpu_avg = 0
                if 'gpu' in stats['resources'] and stats['resources']['gpu']:
                    if 'utilization' in stats['resources']['gpu'] and 'avg' in stats['resources']['gpu']['utilization']:
                        gpu_avg = stats['resources']['gpu']['utilization']['avg']
                        
                cpu_avg = stats['resources']['cpu']['avg'] if 'cpu' in stats['resources'] and 'avg' in stats['resources']['cpu'] else 0
                memory_avg = stats['resources']['memory']['avg'] if 'memory' in stats['resources'] and 'avg' in stats['resources']['memory'] else 0
                throughput = stats['summary']['throughput'] if 'throughput' in stats['summary'] else 0
                
                metrics_simple = ['FPS', 'CPU%', 'GPU%', 'Memory%']
                values_simple = [throughput, cpu_avg, gpu_avg, memory_avg]
                colors_simple = ['blue', 'red', 'green', 'orange']
                
                bars = plt.bar(metrics_simple, values_simple, color=colors_simple, alpha=0.7, 
                              edgecolor='black', linewidth=1)
                plt.title('Performance Metrics', fontsize=14, fontweight='bold')
                plt.ylabel('Value / Percentage')
                plt.xlabel('Metric Type')
                
                # æ·»åŠ æ•°å€¼æ ‡ç­¾
                for bar, value, metric in zip(bars, values_simple, metrics_simple):
                    unit = ' FPS' if metric == 'FPS' else '%'
                    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(values_simple)*0.02,
                            f'{value:.1f}{unit}', ha='center', va='bottom', fontweight='bold')
                
                plt.ylim(0, max(values_simple) * 1.15)
            
            plt.tight_layout(pad=2.0)
            
            # æ·»åŠ æ•´ä½“æ ‡é¢˜å’Œè¯´æ˜Ž
            fig.suptitle(f'YOLO Performance Analysis Report\n'
                        f'Model: {stats["system_info"]["model"]} | Device: {stats["system_info"]["device"]} | '
                        f'Frames: {stats["summary"]["total_frames"]} | Avg FPS: {stats["summary"]["throughput"]:.2f}',
                        fontsize=16, fontweight='bold', y=0.98)
            
            # æ·»åŠ å›¾è¡¨è¯´æ˜Ž
            explanation = (
                "Chart Explanations:\n"
                "â€¢ Top Left: Real-time FPS shows performance consistency over time\n"
                "â€¢ Top Center: Inference time distribution shows processing variability\n" 
                "â€¢ Top Right: Processing breakdown shows time spent in each stage\n"
                "â€¢ Bottom Left: Resource usage shows CPU/GPU utilization over time\n"
                "â€¢ Bottom Center: Average resource utilization comparison\n"
                "â€¢ Bottom Right: Overall performance radar (higher values = better)"
            )
            
            plt.figtext(0.02, 0.02, explanation, fontsize=9, 
                       bbox=dict(boxstyle="round,pad=0.5", facecolor="lightyellow", alpha=0.8))
            
            # ä¿å­˜å›¾è¡¨
            if save_path:
                print(f"   - ä¿å­˜å›¾è¡¨è‡³: {save_path}")
                plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
                
                # éªŒè¯æ–‡ä»¶æ˜¯å¦çœŸçš„ä¿å­˜äº†
                if os.path.exists(save_path):
                    file_size = os.path.getsize(save_path)
                    print(f"   âœ… å›¾è¡¨ä¿å­˜æˆåŠŸ! å¤§å°: {file_size} bytes")
                    
                    # åˆ›å»ºæ€§èƒ½æ‘˜è¦æ–‡æœ¬æ–‡ä»¶
                    summary_path = save_path.replace('_visualization.png', '_performance_summary.txt')
                    self.create_performance_summary(stats, summary_path)
                    
                else:
                    print(f"   âŒ å›¾è¡¨ä¿å­˜å¤±è´¥ï¼Œæ–‡ä»¶ä¸å­˜åœ¨")
            
            # å…³é—­å›¾è¡¨é‡Šæ”¾å†…å­˜
            plt.close()
            
            return fig
            
        except Exception as e:
            print(f"âŒ åˆ›å»ºå¯è§†åŒ–å›¾è¡¨å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def create_performance_summary(self, stats, summary_path):
        """åˆ›å»ºæ€§èƒ½æ‘˜è¦è¯´æ˜Žæ–‡ä»¶"""
        try:
            with open(summary_path, 'w', encoding='utf-8') as f:
                f.write("YOLO Performance Analysis Summary\n")
                f.write("="*50 + "\n\n")
                
                # åŸºæœ¬ä¿¡æ¯
                f.write("Basic Information:\n")
                f.write(f"Model: {stats['system_info']['model']}\n")
                f.write(f"Device: {stats['system_info']['device']}\n")
                f.write(f"Batch Size: {stats['system_info']['batch_size']}\n")
                f.write(f"Total Frames: {stats['summary']['total_frames']}\n")
                f.write(f"Test Duration: {stats['summary']['total_time']:.2f} seconds\n\n")
                
                # æ€§èƒ½æŒ‡æ ‡è§£è¯»
                f.write("Performance Metrics Explanation:\n")
                f.write("-" * 30 + "\n")
                
                fps = stats['summary']['throughput']
                f.write(f"Average FPS: {fps:.2f}\n")
                if fps >= 30:
                    f.write("  â†’ Excellent: Suitable for real-time applications\n")
                elif fps >= 15:
                    f.write("  â†’ Good: Suitable for most video processing tasks\n")
                elif fps >= 5:
                    f.write("  â†’ Fair: Suitable for offline processing\n")
                else:
                    f.write("  â†’ Poor: Consider optimization or hardware upgrade\n")
                
                # å¤„ç†æ—¶é—´åˆ†æž
                inference_time = stats['timing']['inference']['avg']
                preprocess_time = stats['timing']['preprocess']['avg']
                postprocess_time = stats['timing']['postprocess']['avg']
                total_time = inference_time + preprocess_time + postprocess_time
                
                f.write(f"\nProcessing Time Breakdown:\n")
                f.write(f"Preprocessing: {preprocess_time:.2f}ms ({preprocess_time/total_time*100:.1f}%)\n")
                f.write(f"Inference: {inference_time:.2f}ms ({inference_time/total_time*100:.1f}%)\n")
                f.write(f"Postprocessing: {postprocess_time:.2f}ms ({postprocess_time/total_time*100:.1f}%)\n")
                
                # ç“¶é¢ˆåˆ†æž
                f.write(f"\nBottleneck Analysis:\n")
                max_time = max(preprocess_time, inference_time, postprocess_time)
                if max_time == inference_time:
                    f.write("  â†’ GPU/Model inference is the bottleneck\n")
                    f.write("    Suggestions: Consider smaller model or better GPU\n")
                elif max_time == preprocess_time:
                    f.write("  â†’ Image preprocessing is the bottleneck\n")
                    f.write("    Suggestions: Optimize image processing or reduce input size\n")
                else:
                    f.write("  â†’ Post-processing (NMS, etc.) is the bottleneck\n")
                    f.write("    Suggestions: Optimize NMS parameters or use GPU acceleration\n")
                
                # èµ„æºåˆ©ç”¨åˆ†æž
                cpu_avg = stats['resources']['cpu']['avg']
                memory_avg = stats['resources']['memory']['avg']
                
                f.write(f"\nResource Utilization Analysis:\n")
                f.write(f"CPU Usage: {cpu_avg:.1f}%\n")
                if cpu_avg > 300:
                    f.write("  â†’ High multi-core utilization (normal for CPU-heavy tasks)\n")
                elif cpu_avg > 100:
                    f.write("  â†’ Multi-core processing active\n")
                else:
                    f.write("  â†’ Single-core or light processing\n")
                
                f.write(f"Memory Usage: {memory_avg:.1f}%\n")
                if memory_avg > 80:
                    f.write("  â†’ High memory usage, monitor for potential issues\n")
                elif memory_avg > 50:
                    f.write("  â†’ Moderate memory usage\n")
                else:
                    f.write("  â†’ Low memory usage, system has capacity for larger models\n")
                
                # GPUåˆ†æž
                if 'gpu' in stats['resources'] and stats['resources']['gpu']:
                    gpu_util = stats['resources']['gpu']['utilization']['avg']
                    gpu_mem = stats['resources']['gpu']['memory']['avg']
                    
                    f.write(f"GPU Utilization: {gpu_util:.1f}%\n")
                    if gpu_util > 80:
                        f.write("  â†’ Excellent GPU utilization\n")
                    elif gpu_util > 50:
                        f.write("  â†’ Good GPU utilization\n")
                    elif gpu_util > 20:
                        f.write("  â†’ Moderate GPU utilization, consider larger batch size\n")
                    else:
                        f.write("  â†’ Low GPU utilization, may be CPU-bound or small workload\n")
                    
                    f.write(f"GPU Memory: {gpu_mem:.1f}%\n")
                    if gpu_mem > 80:
                        f.write("  â†’ High GPU memory usage, close to limit\n")
                    elif gpu_mem > 50:
                        f.write("  â†’ Moderate GPU memory usage\n")
                    else:
                        f.write("  â†’ Low GPU memory usage, can handle larger models/batches\n")
                else:
                    f.write("GPU: Not available or not utilized\n")
                    f.write("  â†’ Consider using GPU acceleration for better performance\n")
                
                # ä¼˜åŒ–å»ºè®®
                f.write(f"\nOptimization Recommendations:\n")
                f.write("-" * 30 + "\n")
                
                if fps < 15:
                    f.write("â€¢ Performance is below optimal:\n")
                    f.write("  - Try smaller model (yolov8n instead of yolov8s/m/l)\n")
                    f.write("  - Reduce input image size (--imgsz 416 or 320)\n")
                    f.write("  - Increase batch size if GPU memory allows\n")
                
                if cpu_avg > 400:
                    f.write("â€¢ High CPU usage detected:\n")
                    f.write("  - Consider reducing image preprocessing complexity\n")
                    f.write("  - Use GPU for preprocessing if available\n")
                
                if 'gpu' in stats['resources'] and stats['resources']['gpu']:
                    gpu_util = stats['resources']['gpu']['utilization']['avg']
                    if gpu_util < 50:
                        f.write("â€¢ Low GPU utilization:\n")
                        f.write("  - Increase batch size to better utilize GPU\n")
                        f.write("  - Check if preprocessing is creating bottleneck\n")
                
                # ç¨³å®šæ€§åˆ†æž
                inference_std = stats['timing']['inference']['std']
                f.write(f"\nPerformance Stability:\n")
                f.write(f"Inference time standard deviation: {inference_std:.2f}ms\n")
                if inference_std < 5:
                    f.write("  â†’ Very stable performance\n")
                elif inference_std < 15:
                    f.write("  â†’ Good stability\n")
                else:
                    f.write("  â†’ Variable performance, check system load\n")
                
            print(f"   âœ… æ€§èƒ½æ‘˜è¦ä¿å­˜è‡³: {summary_path}")
            
        except Exception as e:
            print(f"   âš ï¸ æ€§èƒ½æ‘˜è¦åˆ›å»ºå¤±è´¥: {e}")
    
    def save_results(self, stats):
        """ä¿å­˜ç»“æžœåˆ°æ–‡ä»¶"""
        if not stats:
            return
        
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        hostname = socket.gethostname()
        base_filename = f"{hostname}_{timestamp}"
        
        # ä¿å­˜TXTæ ¼å¼
        if 'txt' in self.args.output_format:
            txt_filename = f"{base_filename}.txt"
            self.save_txt_report(stats, txt_filename)
            print(f"TXTæŠ¥å‘Šä¿å­˜è‡³: {txt_filename}")
        
        # ä¿å­˜JSONæ ¼å¼
        if 'json' in self.args.output_format:
            json_filename = f"{base_filename}.json"
            with open(json_filename, 'w', encoding='utf-8') as f:
                json.dump(stats, f, indent=2, ensure_ascii=False)
            print(f"JSONæŠ¥å‘Šä¿å­˜è‡³: {json_filename}")
        
        # ä¿å­˜CSVæ ¼å¼
        if 'csv' in self.args.output_format:
            csv_filename = f"{base_filename}.csv"
            self.save_csv_report(stats, csv_filename)
            print(f"CSVæŠ¥å‘Šä¿å­˜è‡³: {csv_filename}")
        
        # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        if self.args.plot:
            if VISUALIZATION_AVAILABLE:
                plot_filename = f"{base_filename}_visualization.png"
                try:
                    self.create_visualizations(stats, plot_filename)
                    print(f"ðŸ“Š å¯è§†åŒ–å›¾è¡¨ä¿å­˜è‡³: {plot_filename}")
                except Exception as e:
                    print(f"âŒ ç”Ÿæˆå›¾è¡¨å¤±è´¥: {e}")
                    print("è¯·è¿è¡Œè¯Šæ–­å·¥å…·æ£€æŸ¥çŽ¯å¢ƒ")
            else:
                print("âŒ æ— æ³•ç”Ÿæˆå›¾è¡¨ï¼šmatplotlib/seabornä¸å¯ç”¨")
                print("å®‰è£…æ–¹æ³•: pip install matplotlib seaborn")
    
    def save_txt_report(self, stats, filename):
        """ä¿å­˜æ–‡æœ¬æ ¼å¼æŠ¥å‘Š"""
        with open(filename, 'w', encoding='utf-8') as f:
            f.write("===== YOLOæ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š =====\n")
            f.write(f"æµ‹è¯•æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"ä¸»æœºå: {stats['system_info']['hostname']}\n")
            f.write(f"æ¨¡åž‹: {stats['system_info']['model']}\n")
            f.write(f"æ•°æ®æº: {stats['system_info']['source']}\n")
            f.write(f"è®¾å¤‡: {stats['system_info']['device']}\n")
            f.write(f"æ‰¹æ¬¡å¤§å°: {stats['system_info']['batch_size']}\n")
            f.write(f"PyTorchç‰ˆæœ¬: {stats['system_info']['torch_version']}\n")
            f.write(f"CUDAå¯ç”¨: {stats['system_info']['cuda_available']}\n")
            f.write(f"è®¾å¤‡åç§°: {stats['system_info']['device_name']}\n\n")
            
            # æ‘˜è¦
            f.write("===== æµ‹è¯•æ‘˜è¦ =====\n")
            f.write(f"æ€»å¸§æ•°: {stats['summary']['total_frames']}\n")
            f.write(f"æ€»æ—¶é—´: {stats['summary']['total_time']:.2f} ç§’\n")
            f.write(f"åžåé‡: {stats['summary']['throughput']:.2f} FPS\n")
            f.write(f"å¹³å‡å¸§æ—¶é—´: {stats['summary']['avg_frame_time']:.2f} æ¯«ç§’\n\n")
            
            # æ—¶é—´è¯¦æƒ…
            f.write("===== æ—¶é—´åˆ†è§£ =====\n")
            for stage, data in stats['timing'].items():
                stage_name = {'preprocess': 'é¢„å¤„ç†', 'inference': 'æŽ¨ç†', 
                             'postprocess': 'åŽå¤„ç†', 'total_per_frame': 'æ€»è®¡æ¯å¸§'}
                f.write(f"{stage_name.get(stage, stage)}:\n")
                f.write(f"  æœ€å°å€¼: {data['min']:.2f} æ¯«ç§’\n")
                f.write(f"  æœ€å¤§å€¼: {data['max']:.2f} æ¯«ç§’\n")
                f.write(f"  å¹³å‡å€¼: {data['avg']:.2f} æ¯«ç§’\n")
                f.write(f"  æ ‡å‡†å·®: {data['std']:.2f} æ¯«ç§’\n\n")
            
            # èµ„æºä½¿ç”¨
            f.write("===== èµ„æºä½¿ç”¨æƒ…å†µ =====\n")
            f.write(f"CPUä½¿ç”¨çŽ‡:\n")
            f.write(f"  æœ€å°å€¼: {stats['resources']['cpu']['min']:.1f}%\n")
            f.write(f"  æœ€å¤§å€¼: {stats['resources']['cpu']['max']:.1f}%\n")
            f.write(f"  å¹³å‡å€¼: {stats['resources']['cpu']['avg']:.1f}%\n\n")
            
            f.write(f"å†…å­˜ä½¿ç”¨çŽ‡:\n")
            f.write(f"  æœ€å°å€¼: {stats['resources']['memory']['min']:.1f}%\n")
            f.write(f"  æœ€å¤§å€¼: {stats['resources']['memory']['max']:.1f}%\n")
            f.write(f"  å¹³å‡å€¼: {stats['resources']['memory']['avg']:.1f}%\n\n")
            
            if 'gpu' in stats['resources']:
                f.write(f"GPUæ˜¾å­˜ä½¿ç”¨çŽ‡:\n")
                f.write(f"  æœ€å°å€¼: {stats['resources']['gpu']['memory']['min']:.1f}%\n")
                f.write(f"  æœ€å¤§å€¼: {stats['resources']['gpu']['memory']['max']:.1f}%\n")
                f.write(f"  å¹³å‡å€¼: {stats['resources']['gpu']['memory']['avg']:.1f}%\n\n")
                
                f.write(f"GPUåˆ©ç”¨çŽ‡:\n")
                f.write(f"  æœ€å°å€¼: {stats['resources']['gpu']['utilization']['min']:.1f}%\n")
                f.write(f"  æœ€å¤§å€¼: {stats['resources']['gpu']['utilization']['max']:.1f}%\n")
                f.write(f"  å¹³å‡å€¼: {stats['resources']['gpu']['utilization']['avg']:.1f}%\n\n")
    
    def save_csv_report(self, stats, filename):
        """ä¿å­˜CSVæ ¼å¼æŠ¥å‘Š"""
        with open(filename, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.writer(f)
            
            # è¡¨å¤´
            writer.writerow(['æŒ‡æ ‡', 'æ•°å€¼', 'å•ä½'])
            
            # æ‘˜è¦æ•°æ®
            writer.writerow(['æ€»å¸§æ•°', stats['summary']['total_frames'], 'å¸§'])
            writer.writerow(['æ€»æ—¶é—´', f"{stats['summary']['total_time']:.2f}", 'ç§’'])
            writer.writerow(['åžåé‡', f"{stats['summary']['throughput']:.2f}", 'FPS'])
            writer.writerow(['å¹³å‡å¸§æ—¶é—´', f"{stats['summary']['avg_frame_time']:.2f}", 'æ¯«ç§’'])
            
            # æŽ¨ç†æ—¶é—´
            writer.writerow(['æŽ¨ç†æœ€å°æ—¶é—´', f"{stats['timing']['inference']['min']:.2f}", 'æ¯«ç§’'])
            writer.writerow(['æŽ¨ç†æœ€å¤§æ—¶é—´', f"{stats['timing']['inference']['max']:.2f}", 'æ¯«ç§’'])
            writer.writerow(['æŽ¨ç†å¹³å‡æ—¶é—´', f"{stats['timing']['inference']['avg']:.2f}", 'æ¯«ç§’'])
            
            # èµ„æºä½¿ç”¨
            writer.writerow(['CPUå¹³å‡ä½¿ç”¨çŽ‡', f"{stats['resources']['cpu']['avg']:.1f}", '%'])
            writer.writerow(['å†…å­˜å¹³å‡ä½¿ç”¨çŽ‡', f"{stats['resources']['memory']['avg']:.1f}", '%'])
            
            if 'gpu' in stats['resources']:
                writer.writerow(['GPUå¹³å‡åˆ©ç”¨çŽ‡', f"{stats['resources']['gpu']['utilization']['avg']:.1f}", '%'])
                writer.writerow(['GPUå¹³å‡æ˜¾å­˜ä½¿ç”¨', f"{stats['resources']['gpu']['memory']['avg']:.1f}", '%'])
    
    def print_summary(self, stats):
        """æ‰“å°è¯¦ç»†æ‘˜è¦åˆ°æŽ§åˆ¶å°"""
        if not stats:
            return
        
        print("\n" + "="*60)
        print("BENCHMARK SUMMARY")
        print("="*60)
        print(f"Total frames processed: {stats['summary']['total_frames']}")
        print(f"Total time elapsed: {stats['summary']['total_time']:.2f} seconds")
        print(f"Throughput: {stats['summary']['throughput']:.2f} frames per second")
        
        # è¯¦ç»†æ—¶é—´ç»Ÿè®¡
        print("\n" + "="*60)
        print("DETAILED METRICS")
        print("="*60)
        print("Preprocess time (ms):")
        print(f"  Min: {stats['timing']['preprocess']['min']:.2f}")
        print(f"  Max: {stats['timing']['preprocess']['max']:.2f}")
        print(f"  Avg: {stats['timing']['preprocess']['avg']:.2f}")
        
        print("\nInference time (ms):")
        print(f"  Min: {stats['timing']['inference']['min']:.2f}")
        print(f"  Max: {stats['timing']['inference']['max']:.2f}")
        print(f"  Avg: {stats['timing']['inference']['avg']:.2f}")
        
        print("\nPostprocess time (ms):")
        print(f"  Min: {stats['timing']['postprocess']['min']:.2f}")
        print(f"  Max: {stats['timing']['postprocess']['max']:.2f}")
        print(f"  Avg: {stats['timing']['postprocess']['avg']:.2f}")
        
        print("\nTotal processing time per frame (ms):")
        print(f"  Min: {stats['timing']['total_per_frame']['min']:.2f}")
        print(f"  Max: {stats['timing']['total_per_frame']['max']:.2f}")
        print(f"  Avg: {stats['timing']['total_per_frame']['avg']:.2f}")
        
        # èµ„æºä½¿ç”¨ç»Ÿè®¡
        print("\n" + "="*60)
        print("RESOURCE UTILIZATION")
        print("="*60)
        
        # CPUä½¿ç”¨çŽ‡è§£é‡Š
        cpu_avg = stats['resources']['cpu']['avg']
        cpu_cores = stats['system_info']['cpu_cores']
        cpu_threads = stats['system_info']['cpu_threads']
        cpu_utilization_percent = (cpu_avg / (cpu_threads * 100)) * 100
        
        print("CPU Usage:")
        print(f"  Min: {stats['resources']['cpu']['min']:.2f}%")
        print(f"  Max: {stats['resources']['cpu']['max']:.2f}%")
        print(f"  Avg: {stats['resources']['cpu']['avg']:.2f}%")
        print(f"  CPU Info: {cpu_cores}æ ¸å¿ƒ/{cpu_threads}çº¿ç¨‹ (æœ€å¤§{cpu_threads * 100}%)")
        print(f"  CPUåˆ©ç”¨çŽ‡: {cpu_utilization_percent:.1f}% çš„æ€»CPUèƒ½åŠ›")
        
        if cpu_avg > 100:
            print(f"  è¯´æ˜Ž: ä½¿ç”¨äº†{cpu_avg/100:.1f}ä¸ªCPUæ ¸å¿ƒï¼Œè¿™æ˜¯æ­£å¸¸çš„å¤šæ ¸å¹¶è¡Œ")
        
        print("\nMemory Usage (%):")
        print(f"  Min: {stats['resources']['memory']['min']:.2f}")
        print(f"  Max: {stats['resources']['memory']['max']:.2f}")
        print(f"  Avg: {stats['resources']['memory']['avg']:.2f}")
        
        if 'gpu' in stats['resources']:
            print("\nGPU Memory Usage (%):")
            print(f"  Min: {stats['resources']['gpu']['memory']['min']:.2f}")
            print(f"  Max: {stats['resources']['gpu']['memory']['max']:.2f}")
            print(f"  Avg: {stats['resources']['gpu']['memory']['avg']:.2f}")
            
            print("\nGPU Utilization (%):")
            print(f"  Min: {stats['resources']['gpu']['utilization']['min']:.2f}")
            print(f"  Max: {stats['resources']['gpu']['utilization']['max']:.2f}")
            print(f"  Avg: {stats['resources']['gpu']['utilization']['avg']:.2f}")
            print(f"  Device: {stats['system_info']['device_name']}")
        else:
            print("\nGPU: Not available")
        
        # æ€§èƒ½è¯„çº§
        fps = stats['summary']['throughput']
        if fps > 30:
            rating = "Excellent ðŸŸ¢"
        elif fps > 15:
            rating = "Good ðŸŸ¡"
        elif fps > 5:
            rating = "Fair ðŸŸ "
        else:
            rating = "Slow ðŸ”´"
        
        print(f"\nPerformance Rating: {rating}")
        print(f"Model: {stats['system_info']['model']}")
        print(f"Device: {stats['system_info']['device']}")
        print(f"Batch Size: {stats['system_info']['batch_size']}")


def main():
    """ä¸»å‡½æ•°"""
    print("YOLOv8 æ€§èƒ½åŸºå‡†æµ‹è¯•å·¥å…· - æ”¹è¿›ç‰ˆ")
    print("-" * 50)
    
    # è§£æžå‚æ•°
    args = parse_arguments()
    
    # åˆ›å»ºåŸºå‡†æµ‹è¯•å·¥å…·
    benchmark = BenchmarkTool(args)
    
    # åŠ è½½æ¨¡åž‹
    benchmark.load_model()
    
    # è¿è¡Œæµ‹è¯•
    success = benchmark.run_benchmark()
    
    if success:
        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        stats = benchmark.calculate_statistics()
        
        # æ‰“å°æ‘˜è¦
        benchmark.print_summary(stats)
        
        # ä¿å­˜ç»“æžœ
        benchmark.save_results(stats)
        
        print("\næµ‹è¯•å®Œæˆ!")
    else:
        print("æµ‹è¯•å¤±è´¥")
        sys.exit(1)


if __name__ == "__main__":
    main()